nohup: ignoring input
Device:  cuda:0
batch_size= 16
load the data
current dir  /home/infres/sbenmabr-22/Point_Transformer
The size of train data is 9843
Load processed data from ./data/modelnet40_normal_resampled/modelnet40_train_1024pts_fps.dat...
The size of test data is 2468
Load processed data from ./data/modelnet40_normal_resampled/modelnet40_test_1024pts_fps.dat...
Creating Model

PointTransformerModel(
  (MLP1): Sequential(
    (0): Linear(in_features=6, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
  )
  (transformer_initial): PointTransformerLayer(
    (linear1): Linear(in_features=32, out_features=64, bias=True)
    (linear2): Linear(in_features=64, out_features=32, bias=True)
    (keys): Linear(in_features=64, out_features=64, bias=False)
    (queries): Linear(in_features=64, out_features=64, bias=False)
    (values): Linear(in_features=64, out_features=64, bias=False)
    (mapping): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )
    (positional_encoding): Sequential(
      (0): Linear(in_features=3, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (down_blocks): ModuleList(
    (0): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (1): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (2): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
  )
  (transformer_blocks): ModuleList(
    (0): PointTransformerLayer(
      (linear1): Linear(in_features=64, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=64, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (1): PointTransformerLayer(
      (linear1): Linear(in_features=128, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=128, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (2): PointTransformerLayer(
      (linear1): Linear(in_features=256, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=256, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
  )
  (MLP2): Sequential(
    (0): Linear(in_features=256, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=40, bias=True)
  )
)
optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3, verbose=True, min_lr=1e-5)
Model training
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [01:09<4:48:23, 69.49s/it]  1%|          | 2/250 [02:14<4:35:19, 66.61s/it]  1%|          | 3/250 [03:15<4:25:16, 64.44s/it]  2%|▏         | 4/250 [04:21<4:26:05, 64.90s/it]  2%|▏         | 5/250 [05:23<4:20:40, 63.84s/it]  2%|▏         | 6/250 [06:28<4:21:16, 64.25s/it]  3%|▎         | 7/250 [07:30<4:16:37, 63.37s/it]  3%|▎         | 8/250 [08:30<4:12:18, 62.56s/it]  4%|▎         | 9/250 [09:32<4:10:25, 62.35s/it]  4%|▍         | 10/250 [10:34<4:08:01, 62.01s/it]  4%|▍         | 11/250 [11:37<4:09:20, 62.60s/it]  5%|▍         | 12/250 [12:52<4:22:20, 66.14s/it]  5%|▌         | 13/250 [14:07<4:31:36, 68.76s/it]  6%|▌         | 14/250 [15:22<4:38:24, 70.78s/it]  6%|▌         | 15/250 [16:37<4:42:19, 72.08s/it]  6%|▋         | 16/250 [17:49<4:41:09, 72.09s/it]  7%|▋         | 17/250 [19:00<4:38:45, 71.78s/it]  7%|▋         | 18/250 [20:12<4:37:37, 71.80s/it]  8%|▊         | 19/250 [21:25<4:37:19, 72.03s/it]  8%|▊         | 20/250 [22:36<4:35:20, 71.83s/it]  8%|▊         | 21/250 [23:48<4:34:16, 71.86s/it]  9%|▉         | 22/250 [25:01<4:34:11, 72.16s/it]  9%|▉         | 23/250 [26:13<4:32:43, 72.09s/it] 10%|▉         | 24/250 [27:27<4:33:57, 72.73s/it] 10%|█         | 25/250 [28:41<4:34:43, 73.26s/it] 10%|█         | 26/250 [29:54<4:32:50, 73.08s/it] 11%|█         | 27/250 [31:00<4:23:07, 70.80s/it] 11%|█         | 28/250 [32:11<4:23:07, 71.12s/it] 12%|█▏        | 29/250 [33:25<4:24:16, 71.75s/it] 12%|█▏        | 30/250 [34:31<4:17:36, 70.26s/it] 12%|█▏        | 31/250 [35:33<4:06:24, 67.51s/it] 13%|█▎        | 32/250 [36:33<3:57:40, 65.41s/it] 13%|█▎        | 33/250 [37:33<3:51:08, 63.91s/it] 14%|█▎        | 34/250 [38:35<3:47:03, 63.07s/it] 14%|█▍        | 35/250 [39:34<3:42:02, 61.97s/it] 14%|█▍        | 36/250 [40:32<3:36:21, 60.66s/it] 15%|█▍        | 37/250 [41:29<3:31:47, 59.66s/it] 15%|█▌        | 38/250 [42:26<3:28:35, 59.03s/it] 16%|█▌        | 39/250 [43:25<3:26:40, 58.77s/it] 16%|█▌        | 40/250 [44:23<3:24:46, 58.51s/it] 16%|█▋        | 41/250 [45:21<3:23:22, 58.39s/it] 17%|█▋        | 42/250 [46:18<3:21:32, 58.13s/it] 17%|█▋        | 43/250 [47:19<3:23:13, 58.91s/it] 18%|█▊        | 44/250 [48:20<3:24:21, 59.52s/it] 18%|█▊        | 45/250 [49:21<3:25:28, 60.14s/it] 18%|█▊        | 46/250 [50:22<3:24:37, 60.18s/it] 19%|█▉        | 47/250 [51:22<3:23:42, 60.21s/it] 19%|█▉        | 48/250 [52:22<3:22:57, 60.28s/it] 20%|█▉        | 49/250 [53:24<3:22:45, 60.53s/it] 20%|██        | 50/250 [54:25<3:22:45, 60.83s/it] 20%|██        | 51/250 [55:26<3:21:33, 60.77s/it] 21%|██        | 52/250 [56:26<3:20:19, 60.70s/it] 21%|██        | 53/250 [57:27<3:19:08, 60.65s/it] 22%|██▏       | 54/250 [58:28<3:18:23, 60.73s/it] 22%|██▏       | 55/250 [59:29<3:18:08, 60.97s/it] 22%|██▏       | 56/250 [1:00:30<3:16:45, 60.86s/it] 23%|██▎       | 57/250 [1:01:30<3:15:27, 60.76s/it] 23%|██▎       | 58/250 [1:02:31<3:13:50, 60.58s/it] 24%|██▎       | 59/250 [1:03:31<3:12:55, 60.61s/it] 24%|██▍       | 60/250 [1:04:33<3:12:55, 60.92s/it] 24%|██▍       | 61/250 [1:05:33<3:11:17, 60.73s/it] 25%|██▍       | 62/250 [1:06:33<3:09:43, 60.55s/it] 25%|██▌       | 63/250 [1:07:33<3:08:17, 60.41s/it] 26%|██▌       | 64/250 [1:08:34<3:07:43, 60.55s/it] 26%|██▌       | 65/250 [1:09:36<3:07:47, 60.91s/it] 26%|██▋       | 66/250 [1:10:34<3:04:34, 60.18s/it] 27%|██▋       | 67/250 [1:11:32<3:00:44, 59.26s/it] 27%|██▋       | 68/250 [1:12:29<2:57:57, 58.67s/it] 28%|██▊       | 69/250 [1:13:26<2:55:52, 58.30s/it] 28%|██▊       | 70/250 [1:14:25<2:55:07, 58.38s/it] 28%|██▊       | 71/250 [1:15:23<2:53:42, 58.22s/it] 29%|██▉       | 72/250 [1:16:20<2:51:57, 57.96s/it] 29%|██▉       | 73/250 [1:17:18<2:50:45, 57.88s/it] 30%|██▉       | 74/250 [1:18:15<2:49:31, 57.79s/it] 30%|███       | 75/250 [1:19:14<2:49:03, 57.96s/it] 30%|███       | 76/250 [1:20:12<2:48:34, 58.13s/it] 31%|███       | 77/250 [1:21:10<2:47:08, 57.97s/it] 31%|███       | 78/250 [1:22:07<2:45:50, 57.85s/it] 32%|███▏      | 79/250 [1:23:05<2:44:42, 57.79s/it] 32%|███▏      | 80/250 [1:24:03<2:43:46, 57.80s/it] 32%|███▏      | 81/250 [1:25:01<2:42:58, 57.86s/it] 33%|███▎      | 82/250 [1:25:59<2:42:02, 57.87s/it] 33%|███▎      | 83/250 [1:26:56<2:40:51, 57.80s/it] 34%|███▎      | 84/250 [1:27:54<2:39:36, 57.69s/it] 34%|███▍      | 85/250 [1:28:51<2:38:27, 57.62s/it] 34%|███▍      | 86/250 [1:29:50<2:37:59, 57.80s/it] 35%|███▍      | 87/250 [1:30:48<2:37:14, 57.88s/it] 35%|███▌      | 88/250 [1:31:45<2:35:47, 57.70s/it] 36%|███▌      | 89/250 [1:32:42<2:34:36, 57.62s/it] 36%|███▌      | 90/250 [1:33:40<2:33:19, 57.50s/it] 36%|███▋      | 91/250 [1:34:38<2:32:49, 57.67s/it] 37%|███▋      | 92/250 [1:35:36<2:32:31, 57.92s/it] 37%|███▋      | 93/250 [1:36:33<2:31:04, 57.73s/it] 38%|███▊      | 94/250 [1:37:31<2:29:52, 57.65s/it] 38%|███▊      | 95/250 [1:38:28<2:28:47, 57.60s/it] 38%|███▊      | 96/250 [1:39:26<2:28:02, 57.68s/it] 39%|███▉      | 97/250 [1:40:25<2:27:39, 57.91s/it] 39%|███▉      | 98/250 [1:41:22<2:26:31, 57.84s/it] 40%|███▉      | 99/250 [1:42:20<2:25:10, 57.68s/it] 40%|████      | 100/250 [1:43:17<2:23:57, 57.58s/it] 40%|████      | 101/250 [1:44:15<2:23:05, 57.62s/it] 41%|████      | 102/250 [1:45:13<2:22:45, 57.87s/it] 41%|████      | 103/250 [1:46:12<2:22:33, 58.19s/it] 42%|████▏     | 104/250 [1:47:10<2:21:16, 58.06s/it] 42%|████▏     | 105/250 [1:48:07<2:19:55, 57.90s/it] 42%|████▏     | 106/250 [1:49:05<2:18:32, 57.72s/it] 43%|████▎     | 107/250 [1:50:03<2:17:52, 57.85s/it] 43%|████▎     | 108/250 [1:51:01<2:17:16, 58.00s/it] 44%|████▎     | 109/250 [1:51:59<2:16:02, 57.89s/it] 44%|████▍     | 110/250 [1:52:56<2:14:52, 57.80s/it] 44%|████▍     | 111/250 [1:53:54<2:13:42, 57.72s/it] 45%|████▍     | 112/250 [1:54:52<2:13:04, 57.86s/it] 45%|████▌     | 113/250 [1:55:51<2:12:39, 58.10s/it] 46%|████▌     | 114/250 [1:56:48<2:11:21, 57.96s/it] 46%|████▌     | 115/250 [1:57:46<2:10:06, 57.83s/it] 46%|████▋     | 116/250 [1:58:43<2:08:59, 57.76s/it] 47%|████▋     | 117/250 [1:59:41<2:07:47, 57.65s/it] 47%|████▋     | 118/250 [2:00:39<2:07:23, 57.90s/it] 48%|████▊     | 119/250 [2:01:38<2:06:33, 57.97s/it] 48%|████▊     | 120/250 [2:02:35<2:05:23, 57.87s/it] 48%|████▊     | 121/250 [2:03:33<2:04:06, 57.73s/it] 49%|████▉     | 122/250 [2:04:30<2:02:50, 57.59s/it] 49%|████▉     | 123/250 [2:05:28<2:02:05, 57.68s/it] 50%|████▉     | 124/250 [2:06:26<2:01:36, 57.91s/it] 50%|█████     | 125/250 [2:07:24<2:00:30, 57.84s/it] 50%|█████     | 126/250 [2:08:21<1:59:11, 57.67s/it] 51%|█████     | 127/250 [2:09:18<1:57:58, 57.55s/it] 51%|█████     | 128/250 [2:10:16<1:56:59, 57.54s/it] 52%|█████▏    | 129/250 [2:11:14<1:56:32, 57.79s/it] 52%|█████▏    | 130/250 [2:12:12<1:55:37, 57.81s/it] 52%|█████▏    | 131/250 [2:13:09<1:54:21, 57.66s/it] 53%|█████▎    | 132/250 [2:14:06<1:53:02, 57.48s/it] 53%|█████▎    | 133/250 [2:15:04<1:52:06, 57.49s/it] 54%|█████▎    | 134/250 [2:16:02<1:51:38, 57.75s/it] 54%|█████▍    | 135/250 [2:17:00<1:50:47, 57.81s/it] 54%|█████▍    | 136/250 [2:17:58<1:49:37, 57.70s/it] 55%|█████▍    | 137/250 [2:18:55<1:48:32, 57.63s/it] 55%|█████▌    | 138/250 [2:19:53<1:47:29, 57.59s/it] 56%|█████▌    | 139/250 [2:20:51<1:46:48, 57.74s/it] 56%|█████▌    | 140/250 [2:21:49<1:46:12, 57.93s/it] 56%|█████▋    | 141/250 [2:22:47<1:44:56, 57.77s/it] 57%|█████▋    | 142/250 [2:23:44<1:43:52, 57.71s/it] 57%|█████▋    | 143/250 [2:24:42<1:42:50, 57.66s/it] 58%|█████▊    | 144/250 [2:25:39<1:41:53, 57.67s/it] 58%|█████▊    | 145/250 [2:26:38<1:41:25, 57.95s/it] 58%|█████▊    | 146/250 [2:27:36<1:40:22, 57.91s/it] 59%|█████▉    | 147/250 [2:28:33<1:39:06, 57.74s/it] 59%|█████▉    | 148/250 [2:29:31<1:38:05, 57.70s/it] 60%|█████▉    | 149/250 [2:30:28<1:37:07, 57.70s/it] 60%|██████    | 150/250 [2:31:27<1:36:29, 57.90s/it] 60%|██████    | 151/250 [2:32:25<1:35:48, 58.06s/it] 61%|██████    | 152/250 [2:33:23<1:34:35, 57.91s/it] 61%|██████    | 153/250 [2:34:20<1:33:20, 57.74s/it] 62%|██████▏   | 154/250 [2:35:18<1:32:20, 57.71s/it] 62%|██████▏   | 155/250 [2:36:16<1:31:34, 57.84s/it] 62%|██████▏   | 156/250 [2:37:15<1:30:58, 58.07s/it] 63%|██████▎   | 157/250 [2:38:12<1:29:47, 57.93s/it] 63%|██████▎   | 158/250 [2:39:10<1:28:44, 57.88s/it] 64%|██████▎   | 159/250 [2:40:07<1:27:39, 57.80s/it] 64%|██████▍   | 160/250 [2:41:05<1:26:43, 57.82s/it] 64%|██████▍   | 161/250 [2:42:04<1:26:06, 58.05s/it] 65%|██████▍   | 162/250 [2:43:02<1:25:04, 58.01s/it] 65%|██████▌   | 163/250 [2:43:59<1:23:49, 57.81s/it] 66%|██████▌   | 164/250 [2:44:57<1:22:44, 57.72s/it] 66%|██████▌   | 165/250 [2:45:54<1:21:41, 57.67s/it] 66%|██████▋   | 166/250 [2:46:53<1:21:05, 57.93s/it] 67%|██████▋   | 167/250 [2:47:51<1:20:21, 58.09s/it] 67%|██████▋   | 168/250 [2:48:49<1:19:12, 57.95s/it] 68%|██████▊   | 169/250 [2:49:46<1:18:02, 57.81s/it] 68%|██████▊   | 170/250 [2:50:44<1:16:52, 57.65s/it] 68%|██████▊   | 171/250 [2:51:42<1:16:04, 57.78s/it] 69%|██████▉   | 172/250 [2:52:40<1:15:28, 58.06s/it] 69%|██████▉   | 173/250 [2:53:38<1:14:13, 57.84s/it] 70%|██████▉   | 174/250 [2:54:35<1:13:05, 57.70s/it] 70%|███████   | 175/250 [2:55:33<1:12:00, 57.61s/it] 70%|███████   | 176/250 [2:56:30<1:11:06, 57.66s/it] 71%|███████   | 177/250 [2:57:29<1:10:29, 57.93s/it] 71%|███████   | 178/250 [2:58:27<1:09:33, 57.97s/it] 72%|███████▏  | 179/250 [2:59:25<1:08:43, 58.08s/it] 72%|███████▏  | 180/250 [3:00:23<1:07:47, 58.11s/it] 72%|███████▏  | 181/250 [3:01:21<1:06:45, 58.05s/it] 73%|███████▎  | 182/250 [3:02:20<1:06:03, 58.28s/it] 73%|███████▎  | 183/250 [3:03:19<1:05:07, 58.31s/it] 74%|███████▎  | 184/250 [3:04:16<1:04:00, 58.20s/it] 74%|███████▍  | 185/250 [3:05:14<1:02:58, 58.12s/it] 74%|███████▍  | 186/250 [3:06:12<1:01:53, 58.02s/it] 75%|███████▍  | 187/250 [3:07:11<1:01:06, 58.19s/it] 75%|███████▌  | 188/250 [3:08:10<1:00:17, 58.35s/it] 76%|███████▌  | 189/250 [3:09:07<59:10, 58.20s/it]   76%|███████▌  | 190/250 [3:10:05<58:05, 58.08s/it] 76%|███████▋  | 191/250 [3:11:03<57:02, 58.02s/it] 77%|███████▋  | 192/250 [3:12:01<56:10, 58.11s/it] 77%|███████▋  | 193/250 [3:13:00<55:26, 58.37s/it] 78%|███████▊  | 194/250 [3:13:58<54:22, 58.26s/it] 78%|███████▊  | 195/250 [3:14:56<53:20, 58.19s/it] 78%|███████▊  | 196/250 [3:15:54<52:18, 58.13s/it] 79%|███████▉  | 197/250 [3:16:53<51:21, 58.15s/it] 79%|███████▉  | 198/250 [3:17:52<50:36, 58.39s/it] 80%|███████▉  | 199/250 [3:18:50<49:31, 58.27s/it] 80%|████████  | 200/250 [3:19:47<48:26, 58.13s/it] 80%|████████  | 201/250 [3:20:45<47:23, 58.02s/it] 81%|████████  | 202/250 [3:21:43<46:24, 58.01s/it] 81%|████████  | 203/250 [3:22:42<45:37, 58.24s/it] 82%|████████▏ | 204/250 [3:23:40<44:36, 58.19s/it] 82%|████████▏ | 205/250 [3:24:38<43:33, 58.08s/it] 82%|████████▏ | 206/250 [3:25:36<42:32, 58.02s/it] 83%|████████▎ | 207/250 [3:26:33<41:32, 57.96s/it] 83%|████████▎ | 208/250 [3:27:32<40:41, 58.12s/it] 84%|████████▎ | 209/250 [3:28:31<39:48, 58.27s/it] 84%|████████▍ | 210/250 [3:29:28<38:45, 58.14s/it] 84%|████████▍ | 211/250 [3:30:26<37:44, 58.07s/it] 85%|████████▍ | 212/250 [3:31:24<36:46, 58.06s/it] 85%|████████▌ | 213/250 [3:32:23<35:54, 58.24s/it] 86%|████████▌ | 214/250 [3:33:22<35:00, 58.36s/it] 86%|████████▌ | 215/250 [3:34:19<33:53, 58.11s/it] 86%|████████▋ | 216/250 [3:35:17<32:48, 57.91s/it] 87%|████████▋ | 217/250 [3:36:14<31:46, 57.76s/it] 87%|████████▋ | 218/250 [3:37:12<30:47, 57.73s/it] 88%|████████▊ | 219/250 [3:38:10<29:54, 57.90s/it] 88%|████████▊ | 220/250 [3:39:07<28:53, 57.77s/it] 88%|████████▊ | 221/250 [3:40:05<27:50, 57.61s/it] 89%|████████▉ | 222/250 [3:41:02<26:50, 57.52s/it] 89%|████████▉ | 223/250 [3:42:00<25:53, 57.54s/it] 90%|████████▉ | 224/250 [3:42:58<25:00, 57.72s/it] 90%|█████████ | 225/250 [3:43:58<24:20, 58.43s/it] 90%|█████████ | 226/250 [3:44:58<23:37, 59.07s/it] 91%|█████████ | 227/250 [3:46:04<23:26, 61.14s/it] 91%|█████████ | 228/250 [3:47:15<23:30, 64.10s/it] 92%|█████████▏| 229/250 [3:48:27<23:12, 66.33s/it] 92%|█████████▏| 230/250 [3:49:40<22:44, 68.25s/it] 92%|█████████▏| 231/250 [3:50:51<21:57, 69.33s/it] 93%|█████████▎| 232/250 [3:52:03<20:59, 69.99s/it] 93%|█████████▎| 233/250 [3:53:14<19:56, 70.40s/it] 94%|█████████▎| 234/250 [3:54:26<18:50, 70.63s/it] 94%|█████████▍| 235/250 [3:55:38<17:46, 71.11s/it] 94%|█████████▍| 236/250 [3:56:50<16:39, 71.42s/it] 95%|█████████▍| 237/250 [3:58:01<15:27, 71.35s/it] 95%|█████████▌| 238/250 [3:59:12<14:15, 71.31s/it] 96%|█████████▌| 239/250 [4:00:24<13:05, 71.39s/it] 96%|█████████▌| 240/250 [4:01:37<11:58, 71.87s/it] 96%|█████████▋| 241/250 [4:02:49<10:48, 72.03s/it] 97%|█████████▋| 242/250 [4:04:01<09:34, 71.84s/it] 97%|█████████▋| 243/250 [4:05:13<08:24, 72.08s/it] 98%|█████████▊| 244/250 [4:06:26<07:13, 72.22s/it] 98%|█████████▊| 245/250 [4:07:39<06:01, 72.37s/it] 98%|█████████▊| 246/250 [4:08:53<04:51, 72.87s/it] 99%|█████████▉| 247/250 [4:10:04<03:37, 72.42s/it] 99%|█████████▉| 248/250 [4:11:15<02:24, 72.14s/it]100%|█████████▉| 249/250 [4:12:26<01:11, 71.81s/it]100%|██████████| 250/250 [4:13:39<00:00, 71.96s/it]100%|██████████| 250/250 [4:13:39<00:00, 60.88s/it]
Epoch: 1, Train Loss: 1.968, Train Acc: 46.622, Test Loss: 1.184, Test Acc: 63.817
Best test accuracy:  63.81685575364668
Epoch: 6, Train Loss: 0.510, Train Acc: 83.603, Test Loss: 0.677, Test Acc: 80.348
Best test accuracy:  81.68557536466774
Epoch: 11, Train Loss: 0.352, Train Acc: 88.337, Test Loss: 0.415, Test Acc: 87.034
Best test accuracy:  87.03403565640194
Epoch: 16, Train Loss: 0.272, Train Acc: 90.572, Test Loss: 0.415, Test Acc: 86.507
Best test accuracy:  87.64181523500811
Epoch: 21, Train Loss: 0.238, Train Acc: 91.883, Test Loss: 0.372, Test Acc: 88.533
Best test accuracy:  88.73581847649919
Epoch: 26, Train Loss: 0.188, Train Acc: 93.396, Test Loss: 0.388, Test Acc: 88.371
Best test accuracy:  89.6677471636953
Epoch 00027: reducing learning rate of group 0 to 4.5000e-03.
Epoch: 31, Train Loss: 0.155, Train Acc: 94.585, Test Loss: 0.388, Test Acc: 88.736
Best test accuracy:  89.6677471636953
Epoch 00031: reducing learning rate of group 0 to 4.0500e-03.
Epoch 00035: reducing learning rate of group 0 to 3.6450e-03.
Epoch: 36, Train Loss: 0.127, Train Acc: 95.438, Test Loss: 0.329, Test Acc: 90.924
Best test accuracy:  90.92382495948137
Epoch 00040: reducing learning rate of group 0 to 3.2805e-03.
Epoch: 41, Train Loss: 0.107, Train Acc: 96.292, Test Loss: 0.352, Test Acc: 89.911
Best test accuracy:  90.92382495948137
Epoch 00044: reducing learning rate of group 0 to 2.9525e-03.
Epoch: 46, Train Loss: 0.096, Train Acc: 96.485, Test Loss: 0.382, Test Acc: 89.587
Best test accuracy:  90.92382495948137
Epoch 00048: reducing learning rate of group 0 to 2.6572e-03.
Epoch: 51, Train Loss: 0.066, Train Acc: 97.663, Test Loss: 0.382, Test Acc: 90.235
Best test accuracy:  90.92382495948137
Epoch 00052: reducing learning rate of group 0 to 2.3915e-03.
Epoch: 56, Train Loss: 0.062, Train Acc: 97.806, Test Loss: 0.388, Test Acc: 89.992
Best test accuracy:  90.92382495948137
Epoch 00056: reducing learning rate of group 0 to 2.1523e-03.
Epoch 00060: reducing learning rate of group 0 to 1.9371e-03.
Epoch: 61, Train Loss: 0.048, Train Acc: 98.334, Test Loss: 0.387, Test Acc: 89.749
Best test accuracy:  90.92382495948137
Epoch 00064: reducing learning rate of group 0 to 1.7434e-03.
Epoch: 66, Train Loss: 0.043, Train Acc: 98.537, Test Loss: 0.390, Test Acc: 89.627
Best test accuracy:  90.92382495948137
Epoch 00068: reducing learning rate of group 0 to 1.5691e-03.
Epoch: 71, Train Loss: 0.038, Train Acc: 98.811, Test Loss: 0.387, Test Acc: 89.951
Best test accuracy:  90.92382495948137
Epoch 00072: reducing learning rate of group 0 to 1.4121e-03.
Epoch: 76, Train Loss: 0.033, Train Acc: 98.913, Test Loss: 0.388, Test Acc: 90.032
Best test accuracy:  90.92382495948137
Epoch 00076: reducing learning rate of group 0 to 1.2709e-03.
Epoch 00080: reducing learning rate of group 0 to 1.1438e-03.
Epoch: 81, Train Loss: 0.031, Train Acc: 98.974, Test Loss: 0.404, Test Acc: 90.519
Best test accuracy:  90.92382495948137
Epoch 00084: reducing learning rate of group 0 to 1.0295e-03.
Epoch: 86, Train Loss: 0.028, Train Acc: 99.106, Test Loss: 0.401, Test Acc: 90.276
Best test accuracy:  90.92382495948137
Epoch 00088: reducing learning rate of group 0 to 9.2651e-04.
Epoch: 91, Train Loss: 0.024, Train Acc: 99.309, Test Loss: 0.393, Test Acc: 90.397
Best test accuracy:  90.92382495948137
Epoch 00092: reducing learning rate of group 0 to 8.3386e-04.
Epoch: 96, Train Loss: 0.021, Train Acc: 99.340, Test Loss: 0.393, Test Acc: 90.316
Best test accuracy:  90.92382495948137
Epoch 00096: reducing learning rate of group 0 to 7.5047e-04.
Epoch 00100: reducing learning rate of group 0 to 6.7543e-04.
Epoch: 101, Train Loss: 0.021, Train Acc: 99.390, Test Loss: 0.401, Test Acc: 90.478
Best test accuracy:  90.92382495948137
Epoch 00104: reducing learning rate of group 0 to 6.0788e-04.
Epoch: 106, Train Loss: 0.017, Train Acc: 99.573, Test Loss: 0.412, Test Acc: 89.992
Best test accuracy:  90.92382495948137
Epoch 00108: reducing learning rate of group 0 to 5.4709e-04.
Epoch: 111, Train Loss: 0.017, Train Acc: 99.472, Test Loss: 0.400, Test Acc: 90.276
Best test accuracy:  90.92382495948137
Epoch 00112: reducing learning rate of group 0 to 4.9239e-04.
Epoch: 116, Train Loss: 0.015, Train Acc: 99.594, Test Loss: 0.404, Test Acc: 90.073
Best test accuracy:  90.92382495948137
Epoch 00116: reducing learning rate of group 0 to 4.4315e-04.
Epoch 00120: reducing learning rate of group 0 to 3.9883e-04.
Epoch: 121, Train Loss: 0.014, Train Acc: 99.594, Test Loss: 0.417, Test Acc: 90.235
Best test accuracy:  90.92382495948137
Epoch 00124: reducing learning rate of group 0 to 3.5895e-04.
Epoch: 126, Train Loss: 0.018, Train Acc: 99.594, Test Loss: 0.416, Test Acc: 90.559
Best test accuracy:  90.92382495948137
Epoch 00128: reducing learning rate of group 0 to 3.2305e-04.
Epoch: 131, Train Loss: 0.014, Train Acc: 99.634, Test Loss: 0.420, Test Acc: 89.911
Best test accuracy:  90.92382495948137
Epoch 00132: reducing learning rate of group 0 to 2.9075e-04.
Epoch: 136, Train Loss: 0.013, Train Acc: 99.675, Test Loss: 0.405, Test Acc: 90.438
Best test accuracy:  90.92382495948137
Epoch 00136: reducing learning rate of group 0 to 2.6167e-04.
Epoch 00140: reducing learning rate of group 0 to 2.3551e-04.
Epoch: 141, Train Loss: 0.011, Train Acc: 99.797, Test Loss: 0.439, Test Acc: 90.032
Best test accuracy:  90.92382495948137
Epoch 00144: reducing learning rate of group 0 to 2.1196e-04.
Epoch: 146, Train Loss: 0.014, Train Acc: 99.573, Test Loss: 0.387, Test Acc: 90.600
Best test accuracy:  90.92382495948137
Epoch 00148: reducing learning rate of group 0 to 1.9076e-04.
Epoch: 151, Train Loss: 0.012, Train Acc: 99.705, Test Loss: 0.411, Test Acc: 90.194
Best test accuracy:  90.92382495948137
Epoch 00152: reducing learning rate of group 0 to 1.7168e-04.
Epoch: 156, Train Loss: 0.012, Train Acc: 99.807, Test Loss: 0.418, Test Acc: 90.600
Best test accuracy:  90.92382495948137
Epoch 00156: reducing learning rate of group 0 to 1.5452e-04.
Epoch 00160: reducing learning rate of group 0 to 1.3906e-04.
Epoch: 161, Train Loss: 0.012, Train Acc: 99.655, Test Loss: 0.418, Test Acc: 89.749
Best test accuracy:  90.92382495948137
Epoch 00164: reducing learning rate of group 0 to 1.2516e-04.
Epoch: 166, Train Loss: 0.012, Train Acc: 99.644, Test Loss: 0.406, Test Acc: 90.073
Best test accuracy:  90.92382495948137
Epoch 00168: reducing learning rate of group 0 to 1.1264e-04.
Epoch: 171, Train Loss: 0.011, Train Acc: 99.746, Test Loss: 0.406, Test Acc: 90.600
Best test accuracy:  90.92382495948137
Epoch 00172: reducing learning rate of group 0 to 1.0138e-04.
Epoch: 176, Train Loss: 0.012, Train Acc: 99.746, Test Loss: 0.408, Test Acc: 90.681
Best test accuracy:  90.92382495948137
Epoch 00176: reducing learning rate of group 0 to 9.1240e-05.
Epoch 00180: reducing learning rate of group 0 to 8.2116e-05.
Epoch: 181, Train Loss: 0.011, Train Acc: 99.756, Test Loss: 0.419, Test Acc: 90.397
Best test accuracy:  90.92382495948137
Epoch 00184: reducing learning rate of group 0 to 7.3904e-05.
Epoch: 186, Train Loss: 0.010, Train Acc: 99.797, Test Loss: 0.423, Test Acc: 90.357
Best test accuracy:  90.92382495948137
Epoch 00188: reducing learning rate of group 0 to 6.6514e-05.
Epoch: 191, Train Loss: 0.012, Train Acc: 99.776, Test Loss: 0.421, Test Acc: 89.951
Best test accuracy:  90.92382495948137
Epoch 00192: reducing learning rate of group 0 to 5.9863e-05.
Epoch: 196, Train Loss: 0.011, Train Acc: 99.756, Test Loss: 0.414, Test Acc: 89.830
Best test accuracy:  90.92382495948137
Epoch 00196: reducing learning rate of group 0 to 5.3876e-05.
Epoch 00200: reducing learning rate of group 0 to 4.8489e-05.
Epoch: 201, Train Loss: 0.010, Train Acc: 99.807, Test Loss: 0.405, Test Acc: 90.235
Best test accuracy:  90.92382495948137
Epoch 00204: reducing learning rate of group 0 to 4.3640e-05.
Epoch: 206, Train Loss: 0.012, Train Acc: 99.756, Test Loss: 0.394, Test Acc: 90.924
Best test accuracy:  90.92382495948137
Epoch 00208: reducing learning rate of group 0 to 3.9276e-05.
Epoch: 211, Train Loss: 0.011, Train Acc: 99.726, Test Loss: 0.418, Test Acc: 90.235
Best test accuracy:  90.92382495948137
Epoch 00212: reducing learning rate of group 0 to 3.5348e-05.
Epoch: 216, Train Loss: 0.010, Train Acc: 99.766, Test Loss: 0.409, Test Acc: 90.721
Best test accuracy:  90.92382495948137
Epoch 00216: reducing learning rate of group 0 to 3.1813e-05.
Epoch 00220: reducing learning rate of group 0 to 2.8632e-05.
Epoch: 221, Train Loss: 0.011, Train Acc: 99.776, Test Loss: 0.417, Test Acc: 90.478
Best test accuracy:  90.92382495948137
Epoch 00224: reducing learning rate of group 0 to 2.5769e-05.
Epoch: 226, Train Loss: 0.011, Train Acc: 99.776, Test Loss: 0.443, Test Acc: 89.303
Best test accuracy:  90.92382495948137
Epoch 00228: reducing learning rate of group 0 to 2.3192e-05.
Epoch: 231, Train Loss: 0.011, Train Acc: 99.675, Test Loss: 0.427, Test Acc: 89.951
Best test accuracy:  90.92382495948137
Epoch 00232: reducing learning rate of group 0 to 2.0873e-05.
Epoch: 236, Train Loss: 0.012, Train Acc: 99.695, Test Loss: 0.414, Test Acc: 90.438
Best test accuracy:  90.92382495948137
Epoch 00236: reducing learning rate of group 0 to 1.8786e-05.
Epoch 00240: reducing learning rate of group 0 to 1.6907e-05.
Epoch: 241, Train Loss: 0.010, Train Acc: 99.746, Test Loss: 0.428, Test Acc: 90.721
Best test accuracy:  90.92382495948137
Epoch 00244: reducing learning rate of group 0 to 1.5216e-05.
Epoch: 246, Train Loss: 0.010, Train Acc: 99.716, Test Loss: 0.407, Test Acc: 90.478
Best test accuracy:  90.92382495948137
Epoch 00248: reducing learning rate of group 0 to 1.3695e-05.
training time 253.0  minutes
