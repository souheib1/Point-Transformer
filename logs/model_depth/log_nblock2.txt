nohup: ignoring input
Device:  cuda:0
batch_size= 16
load the data
current dir  /home/infres/sbenmabr-22/Point_Transformer
The size of train data is 9843
Load processed data from ./data/modelnet40_normal_resampled/modelnet40_train_1024pts_fps.dat...
The size of test data is 2468
Load processed data from ./data/modelnet40_normal_resampled/modelnet40_test_1024pts_fps.dat...
Creating Model

PointTransformerModel(
  (MLP1): Sequential(
    (0): Linear(in_features=6, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
  )
  (transformer_initial): PointTransformerLayer(
    (linear1): Linear(in_features=32, out_features=64, bias=True)
    (linear2): Linear(in_features=64, out_features=32, bias=True)
    (keys): Linear(in_features=64, out_features=64, bias=False)
    (queries): Linear(in_features=64, out_features=64, bias=False)
    (values): Linear(in_features=64, out_features=64, bias=False)
    (mapping): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )
    (positional_encoding): Sequential(
      (0): Linear(in_features=3, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (down_blocks): ModuleList(
    (0): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (1): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
  )
  (transformer_blocks): ModuleList(
    (0): PointTransformerLayer(
      (linear1): Linear(in_features=64, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=64, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (1): PointTransformerLayer(
      (linear1): Linear(in_features=128, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=128, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
  )
  (MLP2): Sequential(
    (0): Linear(in_features=128, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=40, bias=True)
  )
)
optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3, verbose=True, min_lr=1e-5)
Model training
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [01:39<6:52:16, 99.34s/it]  1%|          | 2/250 [02:45<5:30:26, 79.95s/it]  1%|          | 3/250 [03:52<5:03:55, 73.83s/it]  2%|▏         | 4/250 [04:57<4:49:00, 70.49s/it]  2%|▏         | 5/250 [06:03<4:41:06, 68.84s/it]  2%|▏         | 6/250 [07:09<4:36:13, 67.92s/it]  3%|▎         | 7/250 [08:15<4:31:49, 67.12s/it]  3%|▎         | 8/250 [09:20<4:29:04, 66.71s/it]  4%|▎         | 9/250 [10:27<4:27:17, 66.55s/it]  4%|▍         | 10/250 [11:32<4:25:14, 66.31s/it]  4%|▍         | 11/250 [12:38<4:22:41, 65.95s/it]  5%|▍         | 12/250 [13:43<4:21:23, 65.90s/it]  5%|▌         | 13/250 [14:49<4:20:24, 65.93s/it]  6%|▌         | 14/250 [15:54<4:18:08, 65.63s/it]  6%|▌         | 15/250 [16:50<4:05:37, 62.71s/it]  6%|▋         | 16/250 [17:46<3:56:29, 60.64s/it]  7%|▋         | 17/250 [18:41<3:49:20, 59.06s/it]  7%|▋         | 18/250 [19:37<3:43:59, 57.93s/it]  8%|▊         | 19/250 [20:32<3:39:54, 57.12s/it]  8%|▊         | 20/250 [21:28<3:37:28, 56.73s/it]  8%|▊         | 21/250 [22:23<3:34:59, 56.33s/it]  9%|▉         | 22/250 [23:18<3:32:48, 56.00s/it]  9%|▉         | 23/250 [24:14<3:31:11, 55.82s/it] 10%|▉         | 24/250 [25:10<3:30:29, 55.88s/it] 10%|█         | 25/250 [26:06<3:29:25, 55.85s/it] 10%|█         | 26/250 [27:01<3:28:03, 55.73s/it] 11%|█         | 27/250 [27:56<3:26:39, 55.60s/it] 11%|█         | 28/250 [28:52<3:26:00, 55.68s/it] 12%|█▏        | 29/250 [29:48<3:25:17, 55.73s/it] 12%|█▏        | 30/250 [30:43<3:23:46, 55.58s/it] 12%|█▏        | 31/250 [31:39<3:22:43, 55.54s/it] 13%|█▎        | 32/250 [32:35<3:22:02, 55.61s/it] 13%|█▎        | 33/250 [33:30<3:21:28, 55.71s/it] 14%|█▎        | 34/250 [34:26<3:20:03, 55.57s/it] 14%|█▍        | 35/250 [35:21<3:18:53, 55.50s/it] 14%|█▍        | 36/250 [36:17<3:17:59, 55.51s/it] 15%|█▍        | 37/250 [37:13<3:17:41, 55.69s/it] 15%|█▌        | 38/250 [38:08<3:15:54, 55.44s/it] 16%|█▌        | 39/250 [39:03<3:14:43, 55.37s/it] 16%|█▌        | 40/250 [39:58<3:13:50, 55.38s/it] 16%|█▋        | 41/250 [40:53<3:12:46, 55.34s/it] 17%|█▋        | 42/250 [41:48<3:11:29, 55.24s/it] 17%|█▋        | 43/250 [42:44<3:11:19, 55.46s/it] 18%|█▊        | 44/250 [43:39<3:09:51, 55.30s/it] 18%|█▊        | 45/250 [44:35<3:09:26, 55.45s/it] 18%|█▊        | 46/250 [45:30<3:07:50, 55.25s/it] 19%|█▉        | 47/250 [46:25<3:07:07, 55.31s/it] 19%|█▉        | 48/250 [47:21<3:06:15, 55.32s/it] 20%|█▉        | 49/250 [50:20<5:09:24, 92.36s/it] 20%|██        | 50/250 [51:15<4:31:17, 81.39s/it] 20%|██        | 51/250 [52:17<4:09:53, 75.34s/it] 21%|██        | 52/250 [53:09<3:45:29, 68.33s/it] 21%|██        | 53/250 [54:02<3:29:39, 63.85s/it] 22%|██▏       | 54/250 [54:54<3:17:19, 60.41s/it] 22%|██▏       | 55/250 [55:46<3:07:41, 57.75s/it] 22%|██▏       | 56/250 [56:38<3:01:19, 56.08s/it] 23%|██▎       | 57/250 [57:30<2:56:41, 54.93s/it] 23%|██▎       | 58/250 [58:23<2:53:15, 54.14s/it] 24%|██▎       | 59/250 [59:15<2:50:36, 53.60s/it] 24%|██▍       | 60/250 [1:00:07<2:48:02, 53.06s/it] 24%|██▍       | 61/250 [1:00:51<2:38:25, 50.30s/it] 25%|██▍       | 62/250 [1:01:32<2:28:48, 47.49s/it] 25%|██▌       | 63/250 [1:02:13<2:21:59, 45.56s/it] 26%|██▌       | 64/250 [1:02:54<2:16:59, 44.19s/it] 26%|██▌       | 65/250 [1:03:35<2:13:37, 43.34s/it] 26%|██▋       | 66/250 [1:04:16<2:10:41, 42.62s/it] 27%|██▋       | 67/250 [1:04:57<2:08:20, 42.08s/it] 27%|██▋       | 68/250 [1:05:37<2:06:27, 41.69s/it] 28%|██▊       | 69/250 [1:06:18<2:05:05, 41.47s/it] 28%|██▊       | 70/250 [1:06:59<2:03:47, 41.26s/it] 28%|██▊       | 71/250 [1:07:41<2:03:17, 41.32s/it] 29%|██▉       | 72/250 [1:08:22<2:02:23, 41.26s/it] 29%|██▉       | 73/250 [1:09:12<2:09:43, 43.98s/it] 30%|██▉       | 74/250 [1:10:03<2:15:02, 46.04s/it] 30%|███       | 75/250 [1:10:54<2:18:21, 47.44s/it] 30%|███       | 76/250 [1:11:44<2:20:23, 48.41s/it] 31%|███       | 77/250 [1:12:35<2:21:36, 49.11s/it] 31%|███       | 78/250 [1:13:26<2:22:24, 49.68s/it] 32%|███▏      | 79/250 [1:14:17<2:23:00, 50.18s/it] 32%|███▏      | 80/250 [1:15:08<2:22:32, 50.31s/it] 32%|███▏      | 81/250 [1:15:59<2:22:09, 50.47s/it] 33%|███▎      | 82/250 [1:16:49<2:21:25, 50.51s/it] 33%|███▎      | 83/250 [1:17:40<2:20:33, 50.50s/it] 34%|███▎      | 84/250 [1:18:31<2:20:17, 50.71s/it] 34%|███▍      | 85/250 [1:19:20<2:18:18, 50.30s/it] 34%|███▍      | 86/250 [1:20:12<2:18:09, 50.55s/it] 35%|███▍      | 87/250 [1:21:02<2:17:26, 50.59s/it] 35%|███▌      | 88/250 [1:21:53<2:16:33, 50.58s/it] 36%|███▌      | 89/250 [1:22:44<2:16:06, 50.72s/it] 36%|███▌      | 90/250 [1:23:35<2:15:12, 50.71s/it] 36%|███▋      | 91/250 [1:24:25<2:14:14, 50.66s/it] 37%|███▋      | 92/250 [1:25:16<2:13:24, 50.66s/it] 37%|███▋      | 93/250 [1:26:06<2:12:34, 50.67s/it] 38%|███▊      | 94/250 [1:26:57<2:11:53, 50.73s/it] 38%|███▊      | 95/250 [1:27:48<2:11:10, 50.78s/it] 38%|███▊      | 96/250 [1:28:39<2:10:16, 50.75s/it] 39%|███▉      | 97/250 [1:29:30<2:09:17, 50.71s/it] 39%|███▉      | 98/250 [1:30:20<2:08:38, 50.78s/it] 40%|███▉      | 99/250 [1:31:11<2:07:33, 50.68s/it] 40%|████      | 100/250 [1:32:01<2:06:37, 50.65s/it] 40%|████      | 101/250 [1:32:52<2:05:46, 50.65s/it] 41%|████      | 102/250 [1:33:43<2:04:52, 50.63s/it] 41%|████      | 103/250 [1:34:33<2:04:08, 50.67s/it] 42%|████▏     | 104/250 [1:35:24<2:03:28, 50.75s/it] 42%|████▏     | 105/250 [1:36:15<2:02:32, 50.71s/it] 42%|████▏     | 106/250 [1:37:06<2:01:32, 50.64s/it] 43%|████▎     | 107/250 [1:37:56<2:00:45, 50.67s/it] 43%|████▎     | 108/250 [1:38:47<1:59:46, 50.61s/it] 44%|████▎     | 109/250 [1:39:37<1:58:52, 50.58s/it] 44%|████▍     | 110/250 [1:40:28<1:58:20, 50.72s/it] 44%|████▍     | 111/250 [1:41:19<1:57:50, 50.86s/it] 45%|████▍     | 112/250 [1:42:11<1:57:16, 50.99s/it] 45%|████▌     | 113/250 [1:43:02<1:56:17, 50.93s/it] 46%|████▌     | 114/250 [1:43:52<1:55:16, 50.85s/it] 46%|████▌     | 115/250 [1:44:44<1:55:07, 51.17s/it] 46%|████▋     | 116/250 [1:45:35<1:53:58, 51.04s/it] 47%|████▋     | 117/250 [1:46:25<1:52:44, 50.86s/it] 47%|████▋     | 118/250 [1:47:16<1:51:45, 50.80s/it] 48%|████▊     | 119/250 [1:48:07<1:50:50, 50.77s/it] 48%|████▊     | 120/250 [1:48:57<1:49:54, 50.73s/it] 48%|████▊     | 121/250 [1:49:48<1:48:59, 50.70s/it] 49%|████▉     | 122/250 [1:50:39<1:48:06, 50.68s/it] 49%|████▉     | 123/250 [1:51:29<1:47:13, 50.65s/it] 50%|████▉     | 124/250 [1:52:20<1:46:18, 50.62s/it] 50%|█████     | 125/250 [1:53:11<1:45:35, 50.68s/it] 50%|█████     | 126/250 [1:54:01<1:44:39, 50.64s/it] 51%|█████     | 127/250 [1:54:52<1:43:43, 50.60s/it] 51%|█████     | 128/250 [1:55:42<1:43:01, 50.67s/it] 52%|█████▏    | 129/250 [1:56:25<1:37:36, 48.40s/it] 52%|█████▏    | 130/250 [1:57:06<1:32:18, 46.15s/it] 52%|█████▏    | 131/250 [1:57:47<1:28:25, 44.59s/it] 53%|█████▎    | 132/250 [1:58:28<1:25:25, 43.44s/it] 53%|█████▎    | 133/250 [1:59:09<1:23:14, 42.69s/it] 54%|█████▎    | 134/250 [1:59:50<1:21:33, 42.19s/it] 54%|█████▍    | 135/250 [2:00:31<1:20:12, 41.85s/it] 54%|█████▍    | 136/250 [2:01:14<1:19:52, 42.04s/it] 55%|█████▍    | 137/250 [2:02:04<1:23:56, 44.57s/it] 55%|█████▌    | 138/250 [2:02:55<1:26:35, 46.39s/it] 56%|█████▌    | 139/250 [2:03:45<1:28:13, 47.69s/it] 56%|█████▌    | 140/250 [2:04:36<1:29:11, 48.65s/it] 56%|█████▋    | 141/250 [2:05:27<1:29:24, 49.22s/it] 57%|█████▋    | 142/250 [2:06:18<1:29:22, 49.66s/it] 57%|█████▋    | 143/250 [2:07:08<1:29:05, 49.96s/it] 58%|█████▊    | 144/250 [2:07:59<1:28:42, 50.22s/it] 58%|█████▊    | 145/250 [2:08:49<1:28:00, 50.29s/it] 58%|█████▊    | 146/250 [2:09:40<1:27:18, 50.37s/it] 59%|█████▉    | 147/250 [2:10:31<1:26:38, 50.47s/it] 59%|█████▉    | 148/250 [2:11:21<1:25:47, 50.46s/it] 60%|█████▉    | 149/250 [2:12:12<1:25:05, 50.55s/it] 60%|██████    | 150/250 [2:13:03<1:24:19, 50.59s/it] 60%|██████    | 151/250 [2:13:53<1:23:23, 50.54s/it] 61%|██████    | 152/250 [2:14:44<1:22:46, 50.68s/it] 61%|██████    | 153/250 [2:15:35<1:22:12, 50.85s/it] 62%|██████▏   | 154/250 [2:16:27<1:21:33, 50.97s/it] 62%|██████▏   | 155/250 [2:17:17<1:20:33, 50.88s/it] 62%|██████▏   | 156/250 [2:18:08<1:19:41, 50.86s/it] 63%|██████▎   | 157/250 [2:18:59<1:18:42, 50.78s/it] 63%|██████▎   | 158/250 [2:19:50<1:17:57, 50.85s/it] 64%|██████▎   | 159/250 [2:20:41<1:17:08, 50.86s/it] 64%|██████▍   | 160/250 [2:21:31<1:16:03, 50.71s/it] 64%|██████▍   | 161/250 [2:22:22<1:15:11, 50.69s/it] 65%|██████▍   | 162/250 [2:23:12<1:14:22, 50.71s/it] 65%|██████▌   | 163/250 [2:24:03<1:13:39, 50.80s/it] 66%|██████▌   | 164/250 [2:24:54<1:12:39, 50.69s/it] 66%|██████▌   | 165/250 [2:25:44<1:11:49, 50.70s/it] 66%|██████▋   | 166/250 [2:26:35<1:11:07, 50.80s/it] 67%|██████▋   | 167/250 [2:27:26<1:10:17, 50.81s/it] 67%|██████▋   | 168/250 [2:28:17<1:09:24, 50.79s/it] 68%|██████▊   | 169/250 [2:29:08<1:08:30, 50.75s/it] 68%|██████▊   | 170/250 [2:29:59<1:07:49, 50.87s/it] 68%|██████▊   | 171/250 [2:30:49<1:06:52, 50.79s/it] 69%|██████▉   | 172/250 [2:31:41<1:06:08, 50.88s/it] 69%|██████▉   | 173/250 [2:32:31<1:05:17, 50.88s/it] 70%|██████▉   | 174/250 [2:33:22<1:04:20, 50.79s/it] 70%|███████   | 175/250 [2:34:12<1:03:20, 50.68s/it] 70%|███████   | 176/250 [2:35:03<1:02:36, 50.77s/it] 71%|███████   | 177/250 [2:35:54<1:01:47, 50.79s/it] 71%|███████   | 178/250 [2:36:45<1:00:57, 50.80s/it] 72%|███████▏  | 179/250 [2:37:36<1:00:06, 50.79s/it] 72%|███████▏  | 180/250 [2:38:27<59:21, 50.87s/it]   72%|███████▏  | 181/250 [2:39:18<58:25, 50.81s/it] 73%|███████▎  | 182/250 [2:40:18<1:00:48, 53.65s/it] 73%|███████▎  | 183/250 [2:41:26<1:04:39, 57.90s/it] 74%|███████▎  | 184/250 [2:42:36<1:07:42, 61.55s/it] 74%|███████▍  | 185/250 [2:43:47<1:09:46, 64.40s/it] 74%|███████▍  | 186/250 [2:44:58<1:10:52, 66.44s/it] 75%|███████▍  | 187/250 [2:46:09<1:11:15, 67.86s/it] 75%|███████▌  | 188/250 [2:47:20<1:11:03, 68.77s/it] 76%|███████▌  | 189/250 [2:48:30<1:10:21, 69.21s/it] 76%|███████▌  | 190/250 [2:49:41<1:09:39, 69.67s/it] 76%|███████▋  | 191/250 [2:50:52<1:08:49, 69.99s/it] 77%|███████▋  | 192/250 [2:51:57<1:06:23, 68.69s/it] 77%|███████▋  | 193/250 [2:52:56<1:02:22, 65.65s/it] 78%|███████▊  | 194/250 [2:53:54<59:12, 63.44s/it]   78%|███████▊  | 195/250 [2:54:53<56:44, 61.89s/it] 78%|███████▊  | 196/250 [2:55:51<54:41, 60.78s/it] 79%|███████▉  | 197/250 [2:56:49<52:54, 59.90s/it] 79%|███████▉  | 198/250 [2:57:47<51:33, 59.50s/it] 80%|███████▉  | 199/250 [2:58:45<50:16, 59.15s/it] 80%|████████  | 200/250 [2:59:44<49:04, 58.88s/it] 80%|████████  | 201/250 [3:00:42<47:52, 58.62s/it] 81%|████████  | 202/250 [3:01:40<46:47, 58.50s/it] 81%|████████  | 203/250 [3:02:38<45:46, 58.44s/it] 82%|████████▏ | 204/250 [3:03:36<44:45, 58.37s/it] 82%|████████▏ | 205/250 [3:04:34<43:39, 58.22s/it] 82%|████████▏ | 206/250 [3:05:33<42:42, 58.23s/it] 83%|████████▎ | 207/250 [3:06:31<41:46, 58.28s/it] 83%|████████▎ | 208/250 [3:07:29<40:44, 58.21s/it] 84%|████████▎ | 209/250 [3:08:27<39:46, 58.20s/it] 84%|████████▍ | 210/250 [3:09:25<38:48, 58.22s/it] 84%|████████▍ | 211/250 [3:10:24<37:52, 58.26s/it] 85%|████████▍ | 212/250 [3:11:22<36:52, 58.21s/it] 85%|████████▌ | 213/250 [3:12:20<35:54, 58.23s/it] 86%|████████▌ | 214/250 [3:13:18<34:56, 58.24s/it] 86%|████████▌ | 215/250 [3:14:17<33:57, 58.21s/it] 86%|████████▋ | 216/250 [3:15:15<32:56, 58.13s/it] 87%|████████▋ | 217/250 [3:16:13<31:59, 58.17s/it] 87%|████████▋ | 218/250 [3:17:11<31:02, 58.19s/it] 88%|████████▊ | 219/250 [3:18:09<30:03, 58.16s/it] 88%|████████▊ | 220/250 [3:19:07<29:01, 58.05s/it] 88%|████████▊ | 221/250 [3:21:14<38:01, 78.66s/it] 89%|████████▉ | 222/250 [3:22:12<33:52, 72.58s/it] 89%|████████▉ | 223/250 [3:23:11<30:46, 68.39s/it] 90%|████████▉ | 224/250 [3:24:09<28:16, 65.25s/it] 90%|█████████ | 225/250 [3:25:12<26:59, 64.77s/it] 90%|█████████ | 226/250 [3:26:34<27:56, 69.85s/it] 91%|█████████ | 227/250 [3:27:56<28:07, 73.39s/it] 91%|█████████ | 228/250 [3:29:17<27:48, 75.82s/it] 92%|█████████▏| 229/250 [3:30:40<27:16, 77.93s/it] 92%|█████████▏| 230/250 [3:32:01<26:19, 78.96s/it] 92%|█████████▏| 231/250 [3:33:22<25:08, 79.42s/it] 93%|█████████▎| 232/250 [3:34:44<24:02, 80.13s/it] 93%|█████████▎| 233/250 [3:36:05<22:47, 80.42s/it] 94%|█████████▎| 234/250 [3:37:26<21:30, 80.68s/it] 94%|█████████▍| 235/250 [3:38:48<20:14, 80.95s/it] 94%|█████████▍| 236/250 [3:40:08<18:52, 80.90s/it] 95%|█████████▍| 237/250 [3:41:30<17:33, 81.05s/it] 95%|█████████▌| 238/250 [3:42:51<16:11, 80.98s/it] 96%|█████████▌| 239/250 [3:44:12<14:51, 81.06s/it] 96%|█████████▌| 240/250 [3:45:33<13:31, 81.18s/it] 96%|█████████▋| 241/250 [3:46:54<12:10, 81.15s/it] 97%|█████████▋| 242/250 [3:48:17<10:51, 81.46s/it] 97%|█████████▋| 243/250 [3:49:38<09:30, 81.56s/it] 98%|█████████▊| 244/250 [3:50:59<08:08, 81.43s/it] 98%|█████████▊| 245/250 [3:52:02<06:18, 75.64s/it] 98%|█████████▊| 246/250 [3:52:59<04:41, 70.30s/it] 99%|█████████▉| 247/250 [3:54:18<03:38, 72.92s/it] 99%|█████████▉| 248/250 [3:55:39<02:30, 75.11s/it]100%|█████████▉| 249/250 [3:56:58<01:16, 76.40s/it]100%|██████████| 250/250 [3:58:17<00:00, 77.30s/it]100%|██████████| 250/250 [3:58:17<00:00, 57.19s/it]
Epoch: 1, Train Loss: 2.454, Train Acc: 33.171, Test Loss: 2.019, Test Acc: 40.762
Best test accuracy:  40.76175040518638
Epoch: 6, Train Loss: 0.683, Train Acc: 78.553, Test Loss: 0.685, Test Acc: 79.417
Best test accuracy:  79.4165316045381
Epoch: 11, Train Loss: 0.481, Train Acc: 84.720, Test Loss: 0.513, Test Acc: 84.360
Best test accuracy:  84.35980551053484
Epoch: 16, Train Loss: 0.387, Train Acc: 87.362, Test Loss: 0.471, Test Acc: 85.535
Best test accuracy:  86.10210696920583
Epoch: 21, Train Loss: 0.335, Train Acc: 88.804, Test Loss: 0.427, Test Acc: 86.791
Best test accuracy:  87.2771474878444
Epoch: 26, Train Loss: 0.290, Train Acc: 90.389, Test Loss: 0.380, Test Acc: 88.533
Best test accuracy:  88.53322528363047
Epoch: 31, Train Loss: 0.250, Train Acc: 91.496, Test Loss: 0.395, Test Acc: 88.817
Best test accuracy:  88.89789303079417
Epoch 00033: reducing learning rate of group 0 to 4.5000e-03.
Epoch: 36, Train Loss: 0.220, Train Acc: 92.543, Test Loss: 0.440, Test Acc: 87.237
Best test accuracy:  88.89789303079417
Epoch: 41, Train Loss: 0.202, Train Acc: 93.183, Test Loss: 0.369, Test Acc: 88.493
Best test accuracy:  89.38411669367909
Epoch 00042: reducing learning rate of group 0 to 4.0500e-03.
Epoch: 46, Train Loss: 0.172, Train Acc: 94.118, Test Loss: 0.374, Test Acc: 88.857
Best test accuracy:  89.91085899513776
Epoch 00046: reducing learning rate of group 0 to 3.6450e-03.
Epoch 00050: reducing learning rate of group 0 to 3.2805e-03.
Epoch: 51, Train Loss: 0.140, Train Acc: 95.266, Test Loss: 0.367, Test Acc: 88.695
Best test accuracy:  89.91085899513776
Epoch 00054: reducing learning rate of group 0 to 2.9525e-03.
Epoch: 56, Train Loss: 0.135, Train Acc: 95.327, Test Loss: 0.361, Test Acc: 89.060
Best test accuracy:  89.91085899513776
Epoch 00058: reducing learning rate of group 0 to 2.6572e-03.
Epoch: 61, Train Loss: 0.118, Train Acc: 95.814, Test Loss: 0.344, Test Acc: 89.789
Best test accuracy:  89.91085899513776
Epoch: 66, Train Loss: 0.109, Train Acc: 96.170, Test Loss: 0.366, Test Acc: 89.992
Best test accuracy:  90.27552674230145
Epoch 00067: reducing learning rate of group 0 to 2.3915e-03.
Epoch: 71, Train Loss: 0.099, Train Acc: 96.434, Test Loss: 0.382, Test Acc: 89.506
Best test accuracy:  90.27552674230145
Epoch 00071: reducing learning rate of group 0 to 2.1523e-03.
Epoch 00075: reducing learning rate of group 0 to 1.9371e-03.
Epoch: 76, Train Loss: 0.083, Train Acc: 97.084, Test Loss: 0.386, Test Acc: 88.857
Best test accuracy:  90.27552674230145
Epoch 00079: reducing learning rate of group 0 to 1.7434e-03.
Epoch: 81, Train Loss: 0.080, Train Acc: 97.298, Test Loss: 0.366, Test Acc: 90.397
Best test accuracy:  90.3970826580227
Epoch 00083: reducing learning rate of group 0 to 1.5691e-03.
Epoch: 86, Train Loss: 0.073, Train Acc: 97.714, Test Loss: 0.379, Test Acc: 90.032
Best test accuracy:  90.3970826580227
Epoch 00087: reducing learning rate of group 0 to 1.4121e-03.
Epoch: 91, Train Loss: 0.067, Train Acc: 97.775, Test Loss: 0.359, Test Acc: 90.316
Best test accuracy:  90.3970826580227
Epoch 00091: reducing learning rate of group 0 to 1.2709e-03.
Epoch 00095: reducing learning rate of group 0 to 1.1438e-03.
Epoch: 96, Train Loss: 0.063, Train Acc: 97.795, Test Loss: 0.368, Test Acc: 90.032
Best test accuracy:  90.3970826580227
Epoch 00099: reducing learning rate of group 0 to 1.0295e-03.
Epoch: 101, Train Loss: 0.054, Train Acc: 98.253, Test Loss: 0.360, Test Acc: 89.789
Best test accuracy:  90.43760129659644
Epoch 00103: reducing learning rate of group 0 to 9.2651e-04.
Epoch: 106, Train Loss: 0.053, Train Acc: 98.151, Test Loss: 0.391, Test Acc: 90.276
Best test accuracy:  90.43760129659644
Epoch 00107: reducing learning rate of group 0 to 8.3386e-04.
Epoch: 111, Train Loss: 0.051, Train Acc: 98.303, Test Loss: 0.391, Test Acc: 89.546
Best test accuracy:  90.43760129659644
Epoch 00111: reducing learning rate of group 0 to 7.5047e-04.
Epoch 00115: reducing learning rate of group 0 to 6.7543e-04.
Epoch: 116, Train Loss: 0.046, Train Acc: 98.537, Test Loss: 0.390, Test Acc: 89.668
Best test accuracy:  90.43760129659644
Epoch 00119: reducing learning rate of group 0 to 6.0788e-04.
Epoch: 121, Train Loss: 0.047, Train Acc: 98.517, Test Loss: 0.380, Test Acc: 90.721
Best test accuracy:  90.72123176661265
Epoch 00123: reducing learning rate of group 0 to 5.4709e-04.
Epoch: 126, Train Loss: 0.041, Train Acc: 98.862, Test Loss: 0.393, Test Acc: 89.830
Best test accuracy:  90.72123176661265
Epoch 00127: reducing learning rate of group 0 to 4.9239e-04.
Epoch: 131, Train Loss: 0.043, Train Acc: 98.771, Test Loss: 0.392, Test Acc: 89.992
Best test accuracy:  90.72123176661265
Epoch 00131: reducing learning rate of group 0 to 4.4315e-04.
Epoch 00135: reducing learning rate of group 0 to 3.9883e-04.
Epoch: 136, Train Loss: 0.039, Train Acc: 98.700, Test Loss: 0.376, Test Acc: 90.357
Best test accuracy:  90.72123176661265
Epoch 00139: reducing learning rate of group 0 to 3.5895e-04.
Epoch: 141, Train Loss: 0.036, Train Acc: 99.025, Test Loss: 0.408, Test Acc: 89.465
Best test accuracy:  90.72123176661265
Epoch 00143: reducing learning rate of group 0 to 3.2305e-04.
Epoch: 146, Train Loss: 0.038, Train Acc: 98.811, Test Loss: 0.388, Test Acc: 89.587
Best test accuracy:  90.72123176661265
Epoch 00147: reducing learning rate of group 0 to 2.9075e-04.
Epoch: 151, Train Loss: 0.034, Train Acc: 98.984, Test Loss: 0.380, Test Acc: 90.762
Best test accuracy:  90.76175040518639
Epoch 00151: reducing learning rate of group 0 to 2.6167e-04.
Epoch 00155: reducing learning rate of group 0 to 2.3551e-04.
Epoch: 156, Train Loss: 0.031, Train Acc: 99.055, Test Loss: 0.387, Test Acc: 90.235
Best test accuracy:  90.76175040518639
Epoch 00159: reducing learning rate of group 0 to 2.1196e-04.
Epoch: 161, Train Loss: 0.036, Train Acc: 98.984, Test Loss: 0.393, Test Acc: 89.911
Best test accuracy:  90.76175040518639
Epoch 00163: reducing learning rate of group 0 to 1.9076e-04.
Epoch: 166, Train Loss: 0.037, Train Acc: 98.943, Test Loss: 0.388, Test Acc: 89.708
Best test accuracy:  90.76175040518639
Epoch 00167: reducing learning rate of group 0 to 1.7168e-04.
Epoch: 171, Train Loss: 0.032, Train Acc: 99.116, Test Loss: 0.408, Test Acc: 90.194
Best test accuracy:  90.76175040518639
Epoch 00171: reducing learning rate of group 0 to 1.5452e-04.
Epoch 00175: reducing learning rate of group 0 to 1.3906e-04.
Epoch: 176, Train Loss: 0.032, Train Acc: 99.065, Test Loss: 0.417, Test Acc: 89.951
Best test accuracy:  90.76175040518639
Epoch 00179: reducing learning rate of group 0 to 1.2516e-04.
Epoch: 181, Train Loss: 0.031, Train Acc: 99.197, Test Loss: 0.407, Test Acc: 89.506
Best test accuracy:  90.76175040518639
Epoch 00183: reducing learning rate of group 0 to 1.1264e-04.
Epoch: 186, Train Loss: 0.035, Train Acc: 98.903, Test Loss: 0.390, Test Acc: 89.951
Best test accuracy:  90.76175040518639
Epoch 00187: reducing learning rate of group 0 to 1.0138e-04.
Epoch: 191, Train Loss: 0.030, Train Acc: 99.157, Test Loss: 0.398, Test Acc: 90.194
Best test accuracy:  90.76175040518639
Epoch 00191: reducing learning rate of group 0 to 9.1240e-05.
Epoch 00195: reducing learning rate of group 0 to 8.2116e-05.
Epoch: 196, Train Loss: 0.030, Train Acc: 99.187, Test Loss: 0.407, Test Acc: 90.357
Best test accuracy:  90.76175040518639
Epoch 00199: reducing learning rate of group 0 to 7.3904e-05.
Epoch: 201, Train Loss: 0.031, Train Acc: 99.116, Test Loss: 0.405, Test Acc: 90.032
Best test accuracy:  90.76175040518639
Epoch 00203: reducing learning rate of group 0 to 6.6514e-05.
Epoch: 206, Train Loss: 0.030, Train Acc: 99.106, Test Loss: 0.408, Test Acc: 89.668
Best test accuracy:  90.76175040518639
Epoch 00207: reducing learning rate of group 0 to 5.9863e-05.
Epoch: 211, Train Loss: 0.031, Train Acc: 99.116, Test Loss: 0.407, Test Acc: 90.316
Best test accuracy:  90.76175040518639
Epoch 00211: reducing learning rate of group 0 to 5.3876e-05.
Epoch 00215: reducing learning rate of group 0 to 4.8489e-05.
Epoch: 216, Train Loss: 0.029, Train Acc: 99.228, Test Loss: 0.403, Test Acc: 90.113
Best test accuracy:  90.76175040518639
Epoch 00219: reducing learning rate of group 0 to 4.3640e-05.
Epoch: 221, Train Loss: 0.029, Train Acc: 99.218, Test Loss: 0.417, Test Acc: 90.032
Best test accuracy:  90.76175040518639
Epoch 00223: reducing learning rate of group 0 to 3.9276e-05.
Epoch: 226, Train Loss: 0.027, Train Acc: 99.319, Test Loss: 0.400, Test Acc: 89.749
Best test accuracy:  90.76175040518639
Epoch 00227: reducing learning rate of group 0 to 3.5348e-05.
Epoch: 231, Train Loss: 0.030, Train Acc: 99.187, Test Loss: 0.401, Test Acc: 89.870
Best test accuracy:  90.76175040518639
Epoch 00231: reducing learning rate of group 0 to 3.1813e-05.
Epoch 00235: reducing learning rate of group 0 to 2.8632e-05.
Epoch: 236, Train Loss: 0.030, Train Acc: 99.136, Test Loss: 0.406, Test Acc: 90.113
Best test accuracy:  90.76175040518639
Epoch 00239: reducing learning rate of group 0 to 2.5769e-05.
Epoch: 241, Train Loss: 0.031, Train Acc: 99.218, Test Loss: 0.410, Test Acc: 89.830
Best test accuracy:  90.76175040518639
Epoch 00243: reducing learning rate of group 0 to 2.3192e-05.
Epoch: 246, Train Loss: 0.035, Train Acc: 99.208, Test Loss: 0.396, Test Acc: 90.640
Best test accuracy:  90.76175040518639
Epoch 00247: reducing learning rate of group 0 to 2.0873e-05.
training time 238.0  minutes
