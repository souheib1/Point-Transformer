nohup: ignoring input
Device:  cuda:0
batch_size= 16
load the data
current dir  /home/infres/sbenmabr-22/Point_Transformer
The size of train data is 9843
Load processed data from ./data/modelnet40_normal_resampled/modelnet40_train_1024pts_fps.dat...
The size of test data is 2468
Load processed data from ./data/modelnet40_normal_resampled/modelnet40_test_1024pts_fps.dat...
Creating Model

PointTransformerModel(
  (MLP1): Sequential(
    (0): Linear(in_features=6, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=32, bias=True)
  )
  (transformer_initial): PointTransformerLayer(
    (linear1): Linear(in_features=32, out_features=64, bias=True)
    (linear2): Linear(in_features=64, out_features=32, bias=True)
    (keys): Linear(in_features=64, out_features=64, bias=False)
    (queries): Linear(in_features=64, out_features=64, bias=False)
    (values): Linear(in_features=64, out_features=64, bias=False)
    (mapping): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )
    (positional_encoding): Sequential(
      (0): Linear(in_features=3, out_features=64, bias=True)
      (1): ReLU()
      (2): Linear(in_features=64, out_features=64, bias=True)
    )
  )
  (down_blocks): ModuleList(
    (0): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (1): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (2): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (3): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
    (4): TransitionDownLayer(
      (mlp_convs): Sequential(
        (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
      )
    )
  )
  (transformer_blocks): ModuleList(
    (0): PointTransformerLayer(
      (linear1): Linear(in_features=64, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=64, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (1): PointTransformerLayer(
      (linear1): Linear(in_features=128, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=128, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (2): PointTransformerLayer(
      (linear1): Linear(in_features=256, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=256, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (3): PointTransformerLayer(
      (linear1): Linear(in_features=512, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=512, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
    (4): PointTransformerLayer(
      (linear1): Linear(in_features=1024, out_features=64, bias=True)
      (linear2): Linear(in_features=64, out_features=1024, bias=True)
      (keys): Linear(in_features=64, out_features=64, bias=False)
      (queries): Linear(in_features=64, out_features=64, bias=False)
      (values): Linear(in_features=64, out_features=64, bias=False)
      (mapping): Sequential(
        (0): Linear(in_features=64, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
      (positional_encoding): Sequential(
        (0): Linear(in_features=3, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=64, bias=True)
      )
    )
  )
  (MLP2): Sequential(
    (0): Linear(in_features=1024, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=40, bias=True)
  )
)
optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=3, verbose=True, min_lr=1e-5)
Model training
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [01:29<6:10:43, 89.33s/it]  1%|          | 2/250 [02:44<5:34:19, 80.89s/it]  1%|          | 3/250 [04:01<5:26:42, 79.36s/it]  2%|▏         | 4/250 [05:20<5:24:53, 79.24s/it]  2%|▏         | 5/250 [06:40<5:24:23, 79.44s/it]  2%|▏         | 6/250 [08:01<5:24:30, 79.80s/it]  3%|▎         | 7/250 [09:17<5:18:05, 78.54s/it]  3%|▎         | 8/250 [10:33<5:13:48, 77.81s/it]  4%|▎         | 9/250 [11:50<5:11:24, 77.53s/it]  4%|▍         | 10/250 [13:20<5:25:50, 81.46s/it]  4%|▍         | 11/250 [14:55<5:40:54, 85.58s/it]  5%|▍         | 12/250 [16:30<5:51:13, 88.54s/it]  5%|▌         | 13/250 [18:04<5:56:05, 90.15s/it]  6%|▌         | 14/250 [19:33<5:53:06, 89.77s/it]  6%|▌         | 15/250 [21:01<5:49:25, 89.21s/it]  6%|▋         | 16/250 [22:30<5:47:49, 89.18s/it]  7%|▋         | 17/250 [23:59<5:46:12, 89.15s/it]  7%|▋         | 18/250 [25:29<5:45:15, 89.29s/it]  8%|▊         | 19/250 [26:59<5:44:35, 89.50s/it]  8%|▊         | 20/250 [28:32<5:46:55, 90.50s/it]  8%|▊         | 21/250 [30:04<5:47:49, 91.13s/it]  9%|▉         | 22/250 [31:30<5:39:51, 89.44s/it]  9%|▉         | 23/250 [32:58<5:37:30, 89.21s/it] 10%|▉         | 24/250 [34:28<5:36:08, 89.24s/it] 10%|█         | 25/250 [35:49<5:25:37, 86.83s/it] 10%|█         | 26/250 [37:05<5:11:34, 83.46s/it] 11%|█         | 27/250 [38:19<5:00:37, 80.89s/it] 11%|█         | 28/250 [39:34<4:52:02, 78.93s/it] 12%|█▏        | 29/250 [40:48<4:45:36, 77.54s/it] 12%|█▏        | 30/250 [42:00<4:37:46, 75.76s/it] 12%|█▏        | 31/250 [43:11<4:31:29, 74.38s/it] 13%|█▎        | 32/250 [44:22<4:26:26, 73.33s/it] 13%|█▎        | 33/250 [45:33<4:23:02, 72.73s/it] 14%|█▎        | 34/250 [46:45<4:21:05, 72.52s/it] 14%|█▍        | 35/250 [47:58<4:20:27, 72.69s/it] 14%|█▍        | 36/250 [49:14<4:22:18, 73.55s/it] 15%|█▍        | 37/250 [50:30<4:23:41, 74.28s/it] 15%|█▌        | 38/250 [51:45<4:24:02, 74.73s/it] 16%|█▌        | 39/250 [53:01<4:23:59, 75.07s/it] 16%|█▌        | 40/250 [54:16<4:22:23, 74.97s/it] 16%|█▋        | 41/250 [55:32<4:22:16, 75.30s/it] 17%|█▋        | 42/250 [56:48<4:21:37, 75.47s/it] 17%|█▋        | 43/250 [58:04<4:20:49, 75.60s/it] 18%|█▊        | 44/250 [59:19<4:18:44, 75.36s/it] 18%|█▊        | 45/250 [1:00:35<4:18:06, 75.54s/it] 18%|█▊        | 46/250 [1:01:51<4:17:19, 75.68s/it] 19%|█▉        | 47/250 [1:03:07<4:16:13, 75.73s/it] 19%|█▉        | 48/250 [1:04:21<4:14:11, 75.50s/it] 20%|█▉        | 49/250 [1:05:37<4:13:16, 75.60s/it] 20%|██        | 50/250 [1:06:53<4:12:13, 75.67s/it] 20%|██        | 51/250 [1:08:09<4:11:05, 75.71s/it] 21%|██        | 52/250 [1:09:24<4:09:13, 75.52s/it] 21%|██        | 53/250 [1:10:40<4:08:18, 75.62s/it] 22%|██▏       | 54/250 [1:11:53<4:04:22, 74.81s/it] 22%|██▏       | 55/250 [1:13:04<3:59:53, 73.81s/it] 22%|██▏       | 56/250 [1:14:15<3:55:28, 72.83s/it] 23%|██▎       | 57/250 [1:15:26<3:52:15, 72.20s/it] 23%|██▎       | 58/250 [1:16:36<3:49:15, 71.64s/it] 24%|██▎       | 59/250 [1:17:47<3:47:29, 71.46s/it] 24%|██▍       | 60/250 [1:18:57<3:45:13, 71.12s/it] 24%|██▍       | 61/250 [1:20:07<3:42:15, 70.56s/it] 25%|██▍       | 62/250 [1:21:17<3:41:18, 70.63s/it] 25%|██▌       | 63/250 [1:22:28<3:40:09, 70.64s/it] 26%|██▌       | 64/250 [1:23:39<3:39:19, 70.75s/it] 26%|██▌       | 65/250 [1:24:49<3:37:22, 70.50s/it] 26%|██▋       | 66/250 [1:26:00<3:36:34, 70.62s/it] 27%|██▋       | 67/250 [1:27:11<3:35:31, 70.67s/it] 27%|██▋       | 68/250 [1:28:22<3:35:05, 70.91s/it] 28%|██▊       | 69/250 [1:29:34<3:34:36, 71.14s/it] 28%|██▊       | 70/250 [1:30:44<3:32:44, 70.91s/it] 28%|██▊       | 71/250 [1:31:56<3:32:22, 71.19s/it] 29%|██▉       | 72/250 [1:33:07<3:31:18, 71.23s/it] 29%|██▉       | 73/250 [1:34:19<3:30:26, 71.34s/it] 30%|██▉       | 74/250 [1:35:29<3:28:19, 71.02s/it] 30%|███       | 75/250 [1:36:40<3:27:24, 71.11s/it] 30%|███       | 76/250 [1:37:52<3:26:47, 71.31s/it] 31%|███       | 77/250 [1:39:04<3:25:39, 71.33s/it] 31%|███       | 78/250 [1:40:14<3:23:52, 71.12s/it] 32%|███▏      | 79/250 [1:41:25<3:22:29, 71.05s/it] 32%|███▏      | 80/250 [1:42:37<3:21:37, 71.16s/it] 32%|███▏      | 81/250 [1:43:49<3:21:06, 71.40s/it] 33%|███▎      | 82/250 [1:45:00<3:19:54, 71.39s/it] 33%|███▎      | 83/250 [1:46:11<3:18:38, 71.37s/it] 34%|███▎      | 84/250 [1:47:24<3:18:24, 71.71s/it] 34%|███▍      | 85/250 [1:48:35<3:16:37, 71.50s/it] 34%|███▍      | 86/250 [1:49:46<3:15:06, 71.38s/it] 35%|███▍      | 87/250 [1:50:56<3:12:35, 70.89s/it] 35%|███▌      | 88/250 [1:52:07<3:11:58, 71.10s/it] 36%|███▌      | 89/250 [1:53:19<3:11:14, 71.27s/it] 36%|███▌      | 90/250 [1:54:30<3:10:01, 71.26s/it] 36%|███▋      | 91/250 [1:55:40<3:08:05, 70.98s/it] 37%|███▋      | 92/250 [1:56:51<3:06:43, 70.91s/it] 37%|███▋      | 93/250 [1:58:03<3:06:16, 71.19s/it] 38%|███▊      | 94/250 [1:59:14<3:05:16, 71.26s/it] 38%|███▊      | 95/250 [2:00:25<3:03:41, 71.11s/it] 38%|███▊      | 96/250 [2:01:36<3:02:06, 70.95s/it] 39%|███▉      | 97/250 [2:02:47<3:01:10, 71.05s/it] 39%|███▉      | 98/250 [2:03:58<3:00:06, 71.10s/it] 40%|███▉      | 99/250 [2:05:09<2:58:58, 71.12s/it] 40%|████      | 100/250 [2:06:19<2:56:36, 70.64s/it] 40%|████      | 101/250 [2:07:30<2:55:54, 70.83s/it] 41%|████      | 102/250 [2:08:41<2:54:41, 70.82s/it] 41%|████      | 103/250 [2:09:52<2:53:49, 70.95s/it] 42%|████▏     | 104/250 [2:11:03<2:52:21, 70.84s/it] 42%|████▏     | 105/250 [2:12:14<2:51:17, 70.88s/it] 42%|████▏     | 106/250 [2:13:25<2:50:34, 71.07s/it] 43%|████▎     | 107/250 [2:14:37<2:49:40, 71.19s/it] 43%|████▎     | 108/250 [2:15:48<2:48:34, 71.23s/it] 44%|████▎     | 109/250 [2:16:58<2:46:39, 70.92s/it] 44%|████▍     | 110/250 [2:18:10<2:46:02, 71.16s/it] 44%|████▍     | 111/250 [2:19:21<2:44:56, 71.20s/it] 45%|████▍     | 112/250 [2:20:32<2:43:42, 71.18s/it] 45%|████▌     | 113/250 [2:21:42<2:41:41, 70.82s/it] 46%|████▌     | 114/250 [2:22:54<2:41:01, 71.04s/it] 46%|████▌     | 115/250 [2:24:06<2:40:13, 71.21s/it] 46%|████▋     | 116/250 [2:25:17<2:39:01, 71.21s/it] 47%|████▋     | 117/250 [2:26:27<2:37:23, 71.00s/it] 47%|████▋     | 118/250 [2:27:38<2:36:00, 70.91s/it] 48%|████▊     | 119/250 [2:28:49<2:34:57, 70.98s/it] 48%|████▊     | 120/250 [2:30:01<2:34:11, 71.17s/it] 48%|████▊     | 121/250 [2:31:12<2:33:02, 71.18s/it] 49%|████▉     | 122/250 [2:32:22<2:31:05, 70.82s/it] 49%|████▉     | 123/250 [2:33:33<2:29:57, 70.84s/it] 50%|████▉     | 124/250 [2:34:44<2:29:05, 70.99s/it] 50%|█████     | 125/250 [2:35:56<2:28:15, 71.16s/it] 50%|█████     | 126/250 [2:37:06<2:26:26, 70.86s/it] 51%|█████     | 127/250 [2:38:18<2:25:47, 71.11s/it] 51%|█████     | 128/250 [2:39:29<2:24:42, 71.17s/it] 52%|█████▏    | 129/250 [2:40:40<2:23:39, 71.23s/it] 52%|█████▏    | 130/250 [2:41:51<2:22:22, 71.18s/it] 52%|█████▏    | 131/250 [2:43:02<2:20:58, 71.08s/it] 53%|█████▎    | 132/250 [2:44:14<2:20:09, 71.27s/it] 53%|█████▎    | 133/250 [2:45:26<2:19:12, 71.39s/it] 54%|█████▎    | 134/250 [2:46:37<2:18:00, 71.38s/it] 54%|█████▍    | 135/250 [2:47:47<2:15:56, 70.93s/it] 54%|█████▍    | 136/250 [2:48:58<2:14:55, 71.01s/it] 55%|█████▍    | 137/250 [2:50:10<2:14:10, 71.24s/it] 55%|█████▌    | 138/250 [2:51:21<2:13:10, 71.35s/it] 56%|█████▌    | 139/250 [2:52:32<2:11:25, 71.04s/it] 56%|█████▌    | 140/250 [2:53:43<2:10:14, 71.04s/it] 56%|█████▋    | 141/250 [2:54:54<2:09:07, 71.08s/it] 57%|█████▋    | 142/250 [2:56:05<2:08:09, 71.20s/it] 57%|█████▋    | 143/250 [2:57:17<2:06:57, 71.19s/it] 58%|█████▊    | 144/250 [2:58:28<2:06:09, 71.41s/it] 58%|█████▊    | 145/250 [2:59:41<2:05:29, 71.71s/it] 58%|█████▊    | 146/250 [3:00:53<2:04:38, 71.91s/it] 59%|█████▉    | 147/250 [3:02:05<2:03:34, 71.98s/it] 59%|█████▉    | 148/250 [3:03:17<2:01:59, 71.76s/it] 60%|█████▉    | 149/250 [3:04:29<2:01:13, 72.01s/it] 60%|██████    | 150/250 [3:05:41<2:00:03, 72.04s/it] 60%|██████    | 151/250 [3:06:54<1:59:03, 72.15s/it] 61%|██████    | 152/250 [3:08:05<1:57:16, 71.80s/it] 61%|██████    | 153/250 [3:09:17<1:56:16, 71.92s/it] 62%|██████▏   | 154/250 [3:10:29<1:55:13, 72.02s/it] 62%|██████▏   | 155/250 [3:11:42<1:54:11, 72.12s/it] 62%|██████▏   | 156/250 [3:12:52<1:52:22, 71.73s/it] 63%|██████▎   | 157/250 [3:14:05<1:51:23, 71.86s/it] 63%|██████▎   | 158/250 [3:15:17<1:50:30, 72.07s/it] 64%|██████▎   | 159/250 [3:16:30<1:49:28, 72.18s/it] 64%|██████▍   | 160/250 [3:17:41<1:47:50, 71.90s/it] 64%|██████▍   | 161/250 [3:18:52<1:46:27, 71.77s/it] 65%|██████▍   | 162/250 [3:20:04<1:45:21, 71.84s/it] 65%|██████▌   | 163/250 [3:21:17<1:44:21, 71.97s/it] 66%|██████▌   | 164/250 [3:22:28<1:43:07, 71.94s/it] 66%|██████▌   | 165/250 [3:23:40<1:41:47, 71.85s/it] 66%|██████▋   | 166/250 [3:24:53<1:40:52, 72.06s/it] 67%|██████▋   | 167/250 [3:26:04<1:39:37, 72.01s/it] 67%|██████▋   | 168/250 [3:27:16<1:38:19, 71.94s/it] 68%|██████▊   | 169/250 [3:28:28<1:36:51, 71.75s/it] 68%|██████▊   | 170/250 [3:29:40<1:35:49, 71.87s/it] 68%|██████▊   | 171/250 [3:30:52<1:34:46, 71.99s/it] 69%|██████▉   | 172/250 [3:32:05<1:33:47, 72.15s/it] 69%|██████▉   | 173/250 [3:33:15<1:31:51, 71.58s/it] 70%|██████▉   | 174/250 [3:34:26<1:30:37, 71.55s/it] 70%|███████   | 175/250 [3:35:38<1:29:24, 71.53s/it] 70%|███████   | 176/250 [3:36:49<1:28:16, 71.58s/it] 71%|███████   | 177/250 [3:38:00<1:26:42, 71.27s/it] 71%|███████   | 178/250 [3:39:12<1:25:38, 71.37s/it] 72%|███████▏  | 179/250 [3:40:23<1:24:31, 71.43s/it] 72%|███████▏  | 180/250 [3:41:35<1:23:25, 71.50s/it] 72%|███████▏  | 181/250 [3:42:46<1:22:01, 71.33s/it] 73%|███████▎  | 182/250 [3:43:57<1:20:40, 71.18s/it] 73%|███████▎  | 183/250 [3:45:06<1:19:00, 70.75s/it] 74%|███████▎  | 184/250 [3:46:14<1:16:58, 69.98s/it] 74%|███████▍  | 185/250 [3:47:39<1:20:25, 74.24s/it] 74%|███████▍  | 186/250 [3:49:08<1:23:53, 78.65s/it] 75%|███████▍  | 187/250 [3:50:36<1:25:47, 81.71s/it] 75%|███████▌  | 188/250 [3:52:06<1:26:52, 84.07s/it] 76%|███████▌  | 189/250 [3:53:36<1:27:18, 85.88s/it] 76%|███████▌  | 190/250 [3:55:06<1:27:09, 87.16s/it] 76%|███████▋  | 191/250 [3:56:35<1:26:17, 87.75s/it] 77%|███████▋  | 192/250 [3:58:05<1:25:29, 88.44s/it] 77%|███████▋  | 193/250 [3:59:37<1:24:58, 89.44s/it] 78%|███████▊  | 194/250 [4:01:07<1:23:34, 89.55s/it] 78%|███████▊  | 195/250 [4:02:37<1:22:03, 89.53s/it] 78%|███████▊  | 196/250 [4:04:07<1:20:46, 89.75s/it] 79%|███████▉  | 197/250 [4:05:37<1:19:26, 89.93s/it] 79%|███████▉  | 198/250 [4:07:05<1:17:25, 89.34s/it] 80%|███████▉  | 199/250 [4:08:33<1:15:41, 89.04s/it] 80%|████████  | 200/250 [4:10:02<1:14:02, 88.85s/it] 80%|████████  | 201/250 [4:11:31<1:12:37, 88.93s/it] 81%|████████  | 202/250 [4:13:02<1:11:44, 89.68s/it] 81%|████████  | 203/250 [4:14:31<1:09:57, 89.32s/it] 82%|████████▏ | 204/250 [4:15:36<1:02:49, 81.94s/it] 82%|████████▏ | 205/250 [4:16:39<57:21, 76.49s/it]   82%|████████▏ | 206/250 [4:17:44<53:27, 72.90s/it] 83%|████████▎ | 207/250 [4:18:48<50:21, 70.26s/it] 83%|████████▎ | 208/250 [4:19:52<47:49, 68.32s/it] 84%|████████▎ | 209/250 [4:20:55<45:39, 66.82s/it] 84%|████████▍ | 210/250 [4:21:59<43:58, 65.96s/it] 84%|████████▍ | 211/250 [4:23:03<42:27, 65.33s/it] 85%|████████▍ | 212/250 [4:24:08<41:15, 65.15s/it] 85%|████████▌ | 213/250 [4:25:28<43:04, 69.85s/it] 86%|████████▌ | 214/250 [4:27:00<45:45, 76.27s/it] 86%|████████▌ | 215/250 [4:28:35<47:44, 81.85s/it] 86%|████████▋ | 216/250 [4:30:11<48:47, 86.12s/it] 87%|████████▋ | 217/250 [4:31:45<48:46, 88.68s/it] 87%|████████▋ | 218/250 [4:33:21<48:21, 90.68s/it] 88%|████████▊ | 219/250 [4:34:55<47:27, 91.86s/it] 88%|████████▊ | 220/250 [4:36:31<46:26, 92.87s/it] 88%|████████▊ | 221/250 [4:38:05<45:11, 93.49s/it] 89%|████████▉ | 222/250 [4:39:41<43:53, 94.06s/it] 89%|████████▉ | 223/250 [4:41:02<40:32, 90.10s/it] 90%|████████▉ | 224/250 [4:42:15<36:50, 85.00s/it] 90%|█████████ | 225/250 [4:43:28<33:59, 81.57s/it] 90%|█████████ | 226/250 [4:44:42<31:39, 79.17s/it] 91%|█████████ | 227/250 [4:45:55<29:37, 77.27s/it] 91%|█████████ | 228/250 [4:47:08<27:56, 76.21s/it] 92%|█████████▏| 229/250 [4:48:22<26:24, 75.44s/it] 92%|█████████▏| 230/250 [4:49:35<24:52, 74.63s/it] 92%|█████████▏| 231/250 [4:50:48<23:31, 74.29s/it] 93%|█████████▎| 232/250 [4:52:02<22:13, 74.07s/it] 93%|█████████▎| 233/250 [4:53:14<20:49, 73.52s/it] 94%|█████████▎| 234/250 [4:54:28<19:37, 73.57s/it] 94%|█████████▍| 235/250 [4:55:42<18:24, 73.64s/it] 94%|█████████▍| 236/250 [4:56:54<17:06, 73.30s/it] 95%|█████████▍| 237/250 [4:58:08<15:54, 73.41s/it] 95%|█████████▌| 238/250 [4:59:21<14:41, 73.47s/it] 96%|█████████▌| 239/250 [5:00:34<13:25, 73.21s/it] 96%|█████████▌| 240/250 [5:01:48<12:13, 73.34s/it] 96%|█████████▋| 241/250 [5:03:01<11:01, 73.46s/it] 97%|█████████▋| 242/250 [5:04:15<09:46, 73.37s/it] 97%|█████████▋| 243/250 [5:05:29<08:35, 73.66s/it] 98%|█████████▊| 244/250 [5:06:43<07:23, 73.88s/it] 98%|█████████▊| 245/250 [5:07:56<06:07, 73.59s/it] 98%|█████████▊| 246/250 [5:09:10<04:54, 73.51s/it] 99%|█████████▉| 247/250 [5:10:24<03:41, 73.68s/it] 99%|█████████▉| 248/250 [5:11:36<02:26, 73.36s/it]100%|█████████▉| 249/250 [5:14:54<01:50, 110.61s/it]100%|██████████| 250/250 [5:16:19<00:00, 103.09s/it]100%|██████████| 250/250 [5:16:19<00:00, 75.92s/it] 
Epoch: 1, Train Loss: 1.483, Train Acc: 59.149, Test Loss: 0.980, Test Acc: 67.828
Best test accuracy:  67.82820097244732
Epoch: 6, Train Loss: 0.378, Train Acc: 87.687, Test Loss: 0.438, Test Acc: 87.075
Best test accuracy:  87.07455429497568
Epoch: 11, Train Loss: 0.262, Train Acc: 90.978, Test Loss: 0.368, Test Acc: 89.263
Best test accuracy:  89.26256077795786
Epoch 00015: reducing learning rate of group 0 to 4.5000e-03.
Epoch: 16, Train Loss: 0.199, Train Acc: 93.244, Test Loss: 0.385, Test Acc: 88.614
Best test accuracy:  89.26256077795786
Epoch: 21, Train Loss: 0.154, Train Acc: 94.727, Test Loss: 0.401, Test Acc: 89.465
Best test accuracy:  90.76175040518639
Epoch 00022: reducing learning rate of group 0 to 4.0500e-03.
Epoch: 26, Train Loss: 0.120, Train Acc: 95.550, Test Loss: 0.431, Test Acc: 88.736
Best test accuracy:  90.76175040518639
Epoch 00026: reducing learning rate of group 0 to 3.6450e-03.
Epoch 00030: reducing learning rate of group 0 to 3.2805e-03.
Epoch: 31, Train Loss: 0.097, Train Acc: 96.353, Test Loss: 0.385, Test Acc: 88.979
Best test accuracy:  90.76175040518639
Epoch 00034: reducing learning rate of group 0 to 2.9525e-03.
Epoch: 36, Train Loss: 0.058, Train Acc: 97.988, Test Loss: 0.410, Test Acc: 90.194
Best test accuracy:  90.76175040518639
Epoch 00038: reducing learning rate of group 0 to 2.6572e-03.
Epoch: 41, Train Loss: 0.069, Train Acc: 97.531, Test Loss: 0.409, Test Acc: 90.681
Best test accuracy:  90.76175040518639
Epoch 00042: reducing learning rate of group 0 to 2.3915e-03.
Epoch: 46, Train Loss: 0.044, Train Acc: 98.314, Test Loss: 0.406, Test Acc: 89.627
Best test accuracy:  90.76175040518639
Epoch 00046: reducing learning rate of group 0 to 2.1523e-03.
Epoch 00050: reducing learning rate of group 0 to 1.9371e-03.
Epoch: 51, Train Loss: 0.035, Train Acc: 98.761, Test Loss: 0.411, Test Acc: 90.478
Best test accuracy:  90.88330632090762
Epoch 00054: reducing learning rate of group 0 to 1.7434e-03.
Epoch: 56, Train Loss: 0.034, Train Acc: 98.710, Test Loss: 0.441, Test Acc: 90.032
Best test accuracy:  90.88330632090762
Epoch 00058: reducing learning rate of group 0 to 1.5691e-03.
Epoch: 61, Train Loss: 0.024, Train Acc: 99.279, Test Loss: 0.459, Test Acc: 90.762
Best test accuracy:  90.88330632090762
Epoch 00062: reducing learning rate of group 0 to 1.4121e-03.
Epoch: 66, Train Loss: 0.021, Train Acc: 99.319, Test Loss: 0.460, Test Acc: 89.789
Best test accuracy:  90.88330632090762
Epoch 00066: reducing learning rate of group 0 to 1.2709e-03.
Epoch 00070: reducing learning rate of group 0 to 1.1438e-03.
Epoch: 71, Train Loss: 0.016, Train Acc: 99.462, Test Loss: 0.476, Test Acc: 90.519
Best test accuracy:  90.88330632090762
Epoch 00074: reducing learning rate of group 0 to 1.0295e-03.
Epoch: 76, Train Loss: 0.017, Train Acc: 99.482, Test Loss: 0.490, Test Acc: 89.870
Best test accuracy:  90.88330632090762
Epoch 00078: reducing learning rate of group 0 to 9.2651e-04.
Epoch: 81, Train Loss: 0.015, Train Acc: 99.573, Test Loss: 0.467, Test Acc: 90.559
Best test accuracy:  90.88330632090762
Epoch 00082: reducing learning rate of group 0 to 8.3386e-04.
Epoch: 86, Train Loss: 0.013, Train Acc: 99.624, Test Loss: 0.515, Test Acc: 90.113
Best test accuracy:  90.88330632090762
Epoch 00086: reducing learning rate of group 0 to 7.5047e-04.
Epoch 00090: reducing learning rate of group 0 to 6.7543e-04.
Epoch: 91, Train Loss: 0.012, Train Acc: 99.705, Test Loss: 0.484, Test Acc: 90.235
Best test accuracy:  90.88330632090762
Epoch 00094: reducing learning rate of group 0 to 6.0788e-04.
Epoch: 96, Train Loss: 0.011, Train Acc: 99.766, Test Loss: 0.497, Test Acc: 89.951
Best test accuracy:  90.88330632090762
Epoch 00098: reducing learning rate of group 0 to 5.4709e-04.
Epoch: 101, Train Loss: 0.010, Train Acc: 99.746, Test Loss: 0.489, Test Acc: 90.154
Best test accuracy:  91.12641815235008
Epoch 00102: reducing learning rate of group 0 to 4.9239e-04.
Epoch: 106, Train Loss: 0.008, Train Acc: 99.848, Test Loss: 0.502, Test Acc: 89.951
Best test accuracy:  91.12641815235008
Epoch 00106: reducing learning rate of group 0 to 4.4315e-04.
Epoch 00110: reducing learning rate of group 0 to 3.9883e-04.
Epoch: 111, Train Loss: 0.009, Train Acc: 99.827, Test Loss: 0.483, Test Acc: 90.478
Best test accuracy:  91.12641815235008
Epoch 00114: reducing learning rate of group 0 to 3.5895e-04.
Epoch: 116, Train Loss: 0.006, Train Acc: 99.888, Test Loss: 0.481, Test Acc: 90.316
Best test accuracy:  91.12641815235008
Epoch 00118: reducing learning rate of group 0 to 3.2305e-04.
Epoch: 121, Train Loss: 0.009, Train Acc: 99.817, Test Loss: 0.503, Test Acc: 91.045
Best test accuracy:  91.12641815235008
Epoch 00122: reducing learning rate of group 0 to 2.9075e-04.
Epoch: 126, Train Loss: 0.006, Train Acc: 99.868, Test Loss: 0.511, Test Acc: 89.708
Best test accuracy:  91.12641815235008
Epoch 00126: reducing learning rate of group 0 to 2.6167e-04.
Epoch 00130: reducing learning rate of group 0 to 2.3551e-04.
Epoch: 131, Train Loss: 0.005, Train Acc: 99.939, Test Loss: 0.485, Test Acc: 90.154
Best test accuracy:  91.12641815235008
Epoch 00134: reducing learning rate of group 0 to 2.1196e-04.
Epoch: 136, Train Loss: 0.006, Train Acc: 99.878, Test Loss: 0.498, Test Acc: 90.194
Best test accuracy:  91.12641815235008
Epoch 00138: reducing learning rate of group 0 to 1.9076e-04.
Epoch: 141, Train Loss: 0.007, Train Acc: 99.837, Test Loss: 0.531, Test Acc: 89.830
Best test accuracy:  91.12641815235008
Epoch 00142: reducing learning rate of group 0 to 1.7168e-04.
Epoch: 146, Train Loss: 0.006, Train Acc: 99.817, Test Loss: 0.511, Test Acc: 90.194
Best test accuracy:  91.12641815235008
Epoch 00146: reducing learning rate of group 0 to 1.5452e-04.
Epoch 00150: reducing learning rate of group 0 to 1.3906e-04.
Epoch: 151, Train Loss: 0.006, Train Acc: 99.888, Test Loss: 0.523, Test Acc: 90.194
Best test accuracy:  91.12641815235008
Epoch 00154: reducing learning rate of group 0 to 1.2516e-04.
Epoch: 156, Train Loss: 0.009, Train Acc: 99.878, Test Loss: 0.501, Test Acc: 90.316
Best test accuracy:  91.12641815235008
Epoch 00158: reducing learning rate of group 0 to 1.1264e-04.
Epoch: 161, Train Loss: 0.007, Train Acc: 99.837, Test Loss: 0.518, Test Acc: 90.235
Best test accuracy:  91.12641815235008
Epoch 00162: reducing learning rate of group 0 to 1.0138e-04.
Epoch: 166, Train Loss: 0.006, Train Acc: 99.888, Test Loss: 0.506, Test Acc: 90.802
Best test accuracy:  91.12641815235008
Epoch 00166: reducing learning rate of group 0 to 9.1240e-05.
Epoch 00170: reducing learning rate of group 0 to 8.2116e-05.
Epoch: 171, Train Loss: 0.005, Train Acc: 99.898, Test Loss: 0.481, Test Acc: 90.883
Best test accuracy:  91.12641815235008
Epoch 00174: reducing learning rate of group 0 to 7.3904e-05.
Epoch: 176, Train Loss: 0.005, Train Acc: 99.858, Test Loss: 0.510, Test Acc: 90.276
Best test accuracy:  91.12641815235008
Epoch 00178: reducing learning rate of group 0 to 6.6514e-05.
Epoch: 181, Train Loss: 0.005, Train Acc: 99.929, Test Loss: 0.510, Test Acc: 90.235
Best test accuracy:  91.12641815235008
Epoch 00182: reducing learning rate of group 0 to 5.9863e-05.
Epoch: 186, Train Loss: 0.005, Train Acc: 99.878, Test Loss: 0.500, Test Acc: 90.397
Best test accuracy:  91.12641815235008
Epoch 00186: reducing learning rate of group 0 to 5.3876e-05.
Epoch 00190: reducing learning rate of group 0 to 4.8489e-05.
Epoch: 191, Train Loss: 0.004, Train Acc: 99.949, Test Loss: 0.514, Test Acc: 89.911
Best test accuracy:  91.12641815235008
Epoch 00194: reducing learning rate of group 0 to 4.3640e-05.
Epoch: 196, Train Loss: 0.005, Train Acc: 99.909, Test Loss: 0.505, Test Acc: 90.478
Best test accuracy:  91.12641815235008
Epoch 00198: reducing learning rate of group 0 to 3.9276e-05.
Epoch: 201, Train Loss: 0.004, Train Acc: 99.909, Test Loss: 0.517, Test Acc: 89.992
Best test accuracy:  91.12641815235008
Epoch 00202: reducing learning rate of group 0 to 3.5348e-05.
Epoch: 206, Train Loss: 0.005, Train Acc: 99.919, Test Loss: 0.514, Test Acc: 90.559
Best test accuracy:  91.12641815235008
Epoch 00206: reducing learning rate of group 0 to 3.1813e-05.
Epoch 00210: reducing learning rate of group 0 to 2.8632e-05.
Epoch: 211, Train Loss: 0.004, Train Acc: 99.939, Test Loss: 0.507, Test Acc: 90.559
Best test accuracy:  91.12641815235008
Epoch 00214: reducing learning rate of group 0 to 2.5769e-05.
Epoch: 216, Train Loss: 0.005, Train Acc: 99.888, Test Loss: 0.513, Test Acc: 90.032
Best test accuracy:  91.12641815235008
Epoch 00218: reducing learning rate of group 0 to 2.3192e-05.
Epoch: 221, Train Loss: 0.004, Train Acc: 99.949, Test Loss: 0.485, Test Acc: 90.316
Best test accuracy:  91.12641815235008
Epoch 00222: reducing learning rate of group 0 to 2.0873e-05.
Epoch: 226, Train Loss: 0.004, Train Acc: 99.888, Test Loss: 0.509, Test Acc: 90.276
Best test accuracy:  91.12641815235008
Epoch 00226: reducing learning rate of group 0 to 1.8786e-05.
Epoch 00230: reducing learning rate of group 0 to 1.6907e-05.
Epoch: 231, Train Loss: 0.004, Train Acc: 99.898, Test Loss: 0.491, Test Acc: 90.721
Best test accuracy:  91.12641815235008
Epoch 00234: reducing learning rate of group 0 to 1.5216e-05.
Epoch: 236, Train Loss: 0.004, Train Acc: 99.898, Test Loss: 0.494, Test Acc: 90.559
Best test accuracy:  91.12641815235008
Epoch 00238: reducing learning rate of group 0 to 1.3695e-05.
Epoch: 241, Train Loss: 0.011, Train Acc: 99.858, Test Loss: 0.538, Test Acc: 90.276
Best test accuracy:  91.12641815235008
Epoch 00242: reducing learning rate of group 0 to 1.2325e-05.
Epoch: 246, Train Loss: 0.006, Train Acc: 99.898, Test Loss: 0.489, Test Acc: 90.924
Best test accuracy:  91.12641815235008
Epoch 00246: reducing learning rate of group 0 to 1.1093e-05.
Epoch 00250: reducing learning rate of group 0 to 1.0000e-05.
training time 316.0  minutes
